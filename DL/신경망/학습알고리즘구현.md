# 학습 알고리즘 구현하기

**전체**

신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 학습이라 한다. 신경망 학습은 4단계로 수행한다.

> **1단계 - 미니배치**

> 훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실 함수 값을 줄이는 것을 목표로 한다.

> **2단계 - 기울기 산출**

> 미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.

> **3단계 - 매개변수 갱신**

> 경사하강법을 이용하여 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.

> **4단계 - 반복**

> 1~3단계를 반복한다.



위의 순서는 확률적 경사 하강법(SGD)을 이용한 방법이다. 데이터를 미니배치로 무작위로 선정하기 때문에 SGD라고 부른다. 확률적으로 무작위로 골라낸 데이터에 대해 수행하는 경사 하강법이라는 의미다.

### 5.1 미니배치 학습 구현

미니배치 학습이란 훈련 데이터 중 일부를 무작위로 꺼내고, 그 미니배치에 대해서 경사법으로 매개변수를 갱신한다.

**미니배치 구현**

```python
import numpy as np
from dataset.mnist import load_mnist
from two_layer_net import TwoLayerNet

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

train_loss_list = []


# 하이퍼파라미터(사용자가 지정해줘야하는 변수)
iters_num = 10000     # 반복 횟수
train_size = x_train.shape[0]
batch_size = 100      # 미니배치 크기
learning_rate = 0.1
network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

for i in range(iters_num):
    # 미니배치 획득
    batch_mask = np.random.choice(train_size, batch_size)
    x_batch = x_train[batch_mask]
    t_batch = t_train[batch_mask]
    
    # 기울기 계산
    grad = network.numerical_gradient(x_batch, t_batch)
    # grad - network.gradient(x_batch, t_batch) 오차역전법을 통한 성능개선
    
    # 매개변수 갱신
    for key in ('W1', 'b1', 'W2', 'b2'):
        network.params[key] -= learning_rate * grad[key]
        
    # 학습 경과 기록
    loss = network.loss(x_batch, t_batch)
    train_loss_list.append(loss)
```

> **코드 작동 순서**
>
> 1. 60000개의 훈련 데이터에서 임의로 100개의 데이터를 추려낸다.
> 2. 100개의 미니배치를 대상으로 확률적 경사 하강법을 수행해 매개변수를 갱신한다.
> 3. 갱신 횟수를 10000번으로 설정하고, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하고 그 값을 배열에 추가한다.

이 손실 함수의 값이 변화하는 추이를 그래프로 확인해본다.

![img](https://blog.kakaocdn.net/dn/1QBVl/btqIz4FyMJ3/dn3Vpqk51pgNyiIY8WOKOK/img.png)

학습횟수가 늘어나면서 손실 함수의 값이 줄어드는 것을 확인할 수 있다.



## 5.3 시험 데이터로 평가하기

에폭(epoch)은 하나의 단위다.

1에폭은 학습에서 훈련 데이터를 모두 소진했을 때 횟수에 해당한다. 예컨대 훈련 데이터 10000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '소진'한 게 된다. 이 경우 100회가 1에폭이 된다.

코드로 얻은 결과를 그래프로 그려본다.

![img](https://blog.kakaocdn.net/dn/bvSxOt/btqIwDuYElH/F9OnMl25iBJfsNd1sCTlh0/img.png)

> 오버피팅이 일어났을 경우
>
> 훈련이란 훈련 데이터에 대한 정확도를 높이는 방향으로 학습하는 것이니 그 정확도는 에폭을 반복할 수록 높아진다. 반면 훈련 데이터에 지나치게 적응하면, 훈련 데이터와는 다른 데이터에 대해 정확도가 점차 떨어지기 시작한다. 이 순간이 오버피팅이 시작되는 순간이다.
>
> 조기 종료(early stopping)
>
> 시험 데이터에 대한 정확도가 떨어지기 시작하는 순간을 포착해 학습을 중단하면 오버피팅을 예방할 수 있다. 이 기법을 조기 종료라고 한다.



## 6. 정리

1. 기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용한다.
2. 훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가한다.
3. 신경망 학습은 손실 함수를 지표로, 손실 함수의 값이 작아지는 방향으로 가중치 매개변수를 갱신한다.
4. 가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치의 값을 갱신하는 작업을 반복한다.
5. 아주 작은 값을 주었을 때의 차분으로 미분하는 것을 수치 미분이라고 한다.
6. 수치 미분을 이용해 가중치 매개변수의 기울기를 구할 수 있다.
7. 수치 미분을 이용한 계산에는 시간이 걸리지만, 구현은 간단하다. 오차역전파법을 이용하면 기울기를 빠르게 구할 수 있다.
