





# 신경망 학습 - 기울기와 경사하강법

## 4. 기울기 - Gradient

**기울기**는 모든 변수의 편미분을 벡터로 정리한 것을 의미한다.
$$
f(x_0,x_1) = x_0^2 + x_1^2
$$
위 식에서 x0 = 3, x1 = 4일 때, (x0, x1) 양쪽의 편미분을 묶어서 나타낼 수 있다. 이를 **기울기**라고 한다.

```python
def numerical_gradient(f, x): # 함수와 넘파이 배열 x 입력
                              # 넘파이 배열 x의 각 원소에 대해서 수치 미분을 구합니다.
    h = 1e-4                  # 0.0001
    grad = np.zeros_like(x)   # x와 형상이 같고 그 원소가 모두 0인 배열을 생성
    
    for idx in range(x.size): # x의 요소수만큼 반복 
        tmp_val = x[idx]      
        
        # f(x+h) 계산
        x[idx] = tmp_val + h
        fxh1 = f(x)
        
        # f(x-h) 계산
        x[idx] = tmp_val - h
        fxh2 = f(x)
        
        grad[idx] = (fxh1 - fxh2) / (2*h)  # 중심차분
        x[idx] = tmp_val                   # 값 복원
```

numerical_gradient(f, x) 함수의 인수인 f는 함수이고, x는 넘파이 배열이므로 numpy 배열 x의 각 원소에 대해서 수치 미분을 구한다.

**기울기가 의미하는 것을 알아보기 위해 기울기의 결과에 마이너스를 붙인 벡터를 그려본다**

![img](https://blog.kakaocdn.net/dn/TCkEA/btqIqX12ivj/kgkKLdIBGqt4VNkumMdSck/img.png)
$$
f(x_0,x_1) = x_0^2 + x_1^2의 기울기
$$
기울기가 가리키는 방향은 각 위치에서 함수의 출력 값을 가장 크게 줄이는 방향이다. 하지만 기울기가 0인 곳이 반드시 최솟값을 의미하진 않는다.

### 4.1 경사법(경사 하강법) - Gradient descent method

기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수를 학습 시에 찾아야 한다. 최적이란 손실 함수가 최솟값이 될 때의 매개변수 값을 말한다. 이 상황에서 **기울기를 이용해 함수의 최솟값을 찾으려는 것이 경사법**이다.

하지만 기울기가 가리키는 방향이 꼭 최솟값을 가리키는 것은 아니다.

- 극솟값

  국소적인 최솟값, 즉 한정된 범위에서 최솟값

- 안장점

  어느 방향에서 보면 극댓값이고 다른 방향에서 보면 극솟값이 되는 점

- 고원

  복잡하고 찌그러진 모양의 함수라면 평평한 곳으로 파고 들면서 고원이라 하는, 학습이 진행되지 않는 정체기에 빠질 수 있다.
	
	![img](https://blog.kakaocdn.net/dn/dsuSBB/btqIsLGQCXQ/reewzumrmkkylVH23JoaC1/img.png)

$$
f(x_0,x_1) = x_0^2 + x_1^2의 안장점
$$

경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그 다음 이동한 곳에서도 마찬가지로 기울기를 구하고, 또 기울어진 방향으로 나아가기를 반복한다. 이렇게 해서 함수의 값을 점차 줄이는 것을 경사법이라 한다.
$$
x_0 = x_0 - η \frac{∂f}{∂x_0}
$$

$$
x_1 = x_1 - η \frac{∂f}{∂x_1}
$$

η 기호(eta)는 갱신하는 양을 나타낸다. 이를 신경망 학습에서는 학습률(learning rate)라고 한다. 한번의 학습으로 멀만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하는냐를 정하는 것이 학습률이다.

 **경사 하강법 구현**

```python
def gradient_descent(f, init_x, lr=0.01, step_num=100):
    # f는 최적화하려는 함수, init_x는 초깃값, lr은 학습률, step_num은 반복 횟수를 의미합니다.
    x = init_x
    for i in range(step_num):
        grad = numerical_gradient(f, x) # 함수의 기울기 구하기
        x -= lr * grad
    
    return x
```

$$
경사법으로\,f(x_0,x_1) = x_0^2 + x_1^2 의 최솟값 구하기
$$

```python
def function_2(x):
    return x[0]**2 + x[1]**2

init_x = np.array([-3.0, 4.0])

gradient_descent(function_2, init_x=init_x, lr=0,1, step_num=100)
>>> array([-6.111107e-10, 8.148143e-10]) # 0에 가까운 것을 확인할 수 있습니다.
```



<img src="https://blog.kakaocdn.net/dn/Tj3yi/btqIz4E87Fs/6ax14kchQbpRIznQKSjYi1/img.png" alt="img" style="zoom:50%;" />

​																		                   경사법에 의한 갱신 과정



### 4.2 신경망에서의 기울기

신경망 학습에서도 기울기를 구해야 한다. 여기서 말하는 기울기는 가중치 매개변수에 대한 손실함수의 기울기다. 예를 들어 형상이 2*3, 가중치가 W, 손실 함수가 L인 신경망을 생각해본다. 이 경우 경사는 아래와 같다.



![img](https://blog.kakaocdn.net/dn/cbNzCT/btqIzX0DNfH/0X4xgC5sOJcO25XFxFviZK/img.png)

**간단한 신경망 구현**

```python
class simpleNet:
    def __init__(self):
        self.W = np.random.randn(2,3) # 평균 0, 표준편차 1인 가우시안 정규분포 난수를 2X3 배열 생성
        
    def predict(self, x):             # 예측 수행
        return np.dot(x, self.W)      # x와 self.W 내적
        
    def loss(self, x, t):             # x는 입력, t는 정답 레이블
        z = self.predict(x)
        y = softmax(z)
        loss = cross_entropy_error(y, t)  # 교차 엔트로피 오차 이용
        
        return loss
        
net = simpleNet()
print(net.W)                             # 가중치 매개변수
>>> [[ 0.47355232, 0.9977393, 0.846690]
     [ 0.85557411, 0.0356366, 0.694220]] # 평균 0, 표준편차 1 정규분포 난수 생성
     
x = np.array([0.6, 0.9])

p = net.perdict(x)
print(p)
>>> [1.05414 0.63071 1.13280]  # 소프트 맥스함수를 거치지 않아 확률로 나오지 않았습니다.
                               # 어차피 최댓값의 인덱스를 구해야 하므로 상관없습니다.

print( np.argmax(p) )          # 최댓값의 인덱스
>>> 2

t = np.array([0, 0, 1])        # 정답 레이블
print(net loss(x, t))          # 손실 함수 구하기
>>> 0.9280685366
```

손실함수까지 구했으니 기울기를 구해본다.

```python
def f(W):                           # net.W를 인수로 받아 손실 함수를 계산하는 새로운 함수 정의
    return net.loss(x, t)
    
dW = numerical_gradient(f, net.W)   # 손실 함수의 기울기

print(dW)
>>> [[0.22 0.14 -0.36]
     [0.32 0.21 -0.54]]   
                          # dW가 0.2의 의미는 w을 h만큼 늘리면 손실 함수의 값은 0.2h만큼 증가합니다.
                          # 손실 함수를 줄인다는 관점으로 -0.54는 양의 방향으로 갱신하고
                          # 0.2는 음의 방향으로 갱신해줘야 합니다.
```

신경망의 기울기를 구한 다음에는 경사법에 따라 가중치 매개변수를 갱신하면 된다.











