# 신경망(2) - 출력층 설계와 MNIST 구현



## 5. 출력층 설계하기

신경망은 분류와 회귀에 모두 이용할 수 있다. 둘 중 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라진다. 일반적으로 회귀에는 항등함수를 분류에는 소프트맥스 함수를 사용한다.

기계학습 문제는 분류와 회귀로 나뉜다.

​	분류(classification): 데이터가 어느 클래스에 속하냐 문제

​	회귀(regression): 입력데이터에서 (연속적인) 수치를 예측하는 문제



### 5.1 항등 함수와 소프트맥스 함수 구현하기

**항등 함수**는 입력을 그대로 출력한다.

![img](https://blog.kakaocdn.net/dn/eUjKf7/btqIqm63Nuv/ME4bxzT4b750cR7DG6UsGk/img.png)

분류에 사용되는 소프트맥스 함수의 식은 다음과 같다

![img](https://blog.kakaocdn.net/dn/Ak0sW/btqIglvbeRU/dATSYwj87ldZbW6nbnzcs0/img.png)

n은 출력층의 뉴런 수, yk는 그 중 k번째 출력을 뜻한다.

소프트맥스 함수의 분자는 입력 신호 ak의 지수 함수, 분모는 모든 입ㄹ 신호의 지수 합수 합으로 구성된다.

![img](https://blog.kakaocdn.net/dn/eFtPFW/btqH8cyKs6t/zXEvXZMF63aeGW9NBbOfk1/img.png)

소프트맥스 함수 구현

```python
a = np.array([0.3, 2.9, 4.0])

def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```

### 5.2 소프트맥스 함수 구현 시 주의점

컴퓨터 계산할 때는 오버플로우 문제가 있다. 

**오버플로(overflow)**란?
컴퓨터는 수를 4바이트나 8바이트와 같이 크기가 유한한 데이터로 다룹니다. 다시 말해 표현할 수 있는 수의 범위가 한정되어 너무 큰 값은 표현할 수 없다는 문제가 발생합니다. 이것을 오버플로라 하며, 컴퓨터로 수치를 계산할 때 주의할 점입니다.

이 문제를 해결하도록 소프트맥스 함수 구현을 개선해본다.

![img](https://blog.kakaocdn.net/dn/vtFEj/btqIasamMMV/Kr3Uy9AUywM255PCjtntk0/img.png)

이 식이 말하는 것은 소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더하거나 빼도 결과가 바뀌지 않는 다는 것이다. 오버플로를 막을 목적으로 C를 입력신호 중 최댓값을 이용해 빼주는 것이 일반적이다.

```python
a = np.array([1010, 1000, 900])
print(np.exp(a) / np.sum(a)) # 소프트 맥스 함수의 계산
>>> array( [ nan, nan, nan] ) # 제대로 계산되지 않습니다.

# 오버플로 문제를 개선해보겠습니다.
c = np.max(a) # c는 입력의 최댓값을 이용합니다.
print( a - c )
>>> array( [0, -10, -20] )

print( np.exp(a - c) / np.sum( np.exp(a - c) )
>>> array( [ 9.9995460e-01, 4.53978686e-05, 2.06106005e-09 ] ) # 제대로 계산이 되는것을 확인할 수 있습니다.
```

이를 바탕으로 소프트맥스 함수를 구현

```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a-c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```



### 5.3 소프트맥스 함수의 특징

softmax() 함수를 사용하면 신경망의 출력은 다음과 같이 계산할 수 있다.

```python
a = np.array([0.3,2.9,4.0])
y = softmax(a)

print(y)
>>> [ 0.01821128 0.24519181 0.73659691]

print(np.sum(y))
>>> 1.0
```

소프트맥스 함수의 특징

1. 출력 총합이 1
2. 소프트 맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다.
3. 현업에서 출력층의 소프트맥스 함수는 생략하는 것이 일반적이다.



### 5.4 출력층의 뉴런 수 정하기

출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야 한다. 분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적이다. 예를 들어 입력 이미지를 숫자 0부터 9중 하나로 분류하는 문제라면 출력층의 뉴런을 10개로 설정한다.

![img](https://blog.kakaocdn.net/dn/4eyUW/btqIl9tOr7w/mDRBDoqKoxLSjdMCqLTTI0/img.png)

​																				출력층의 뉴런은 각 숫자에 대응한다.



출력층 뉴런은 위에서부터 차례로 숫자 0~9에 대응하며, 뉴런의 짙은 농도가 해당 뉴런의 출력 값의 크기를 의미한다.



## 6. 손글씨 숫자 인식

기계학습과 마찬가지로 신경망도 두 단계를 거쳐 문제를 해결한다. 먼저 훈련 데이터를 사용해 가중치 매개변수를 학습하고, 추론 단계에서는 앞서 학습한 매개변수를 사용하여 입력데이터를 분류한다.



###  6.1 MNIST 데이터셋

![img](https://blog.kakaocdn.net/dn/b5ZiKa/btqIqlUVujJ/aW1wh45KNhzxQ100WYlUxk/img.png)

**load_mnist함수**

MNIST 데이터를 (train_image, train_label), (test_image, test_label) 형식으로 반환

또한 이미지 파일을 넘파이 배열로 변환하여 배열 연산을 가능하게 한다.

인수로는 normalize, flatten, one_hot_label 세가지를 설정할 수 있다.

1. normalize

   입력 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지를 정한다. False로 설정하면 입력 이미지의 픽셀은 원래 값 그대로 0~255 사이의 값을 유지한다.

2. flatten

   입력 이미지를 1차원 배열로 만들지 정한다. False로 설정하면 입력 이미지를 1 * 28 * 28의 3차원 배열로, True로 설명하면 28*28=784개의 원소로 이뤄진 1차원 배열로 저장한다.

2. one_hot_label

   레이블을 원-핫 인코딩 형태로 저장할지를 정한다.

   원-핫 인코딩이란, 예를 들어 [0,0,1,0,0,0,0,0,0]처럼 정답을 뜻하는 원소만 1이고 나머지는 모두 0인 배열입니다. False로 설정하면 '7'이나 '2'와 같이 숫자 형태의 레이블을 저장한다.

**pickle 기능**

프로그램 실행 중에 특정 객체를 파일로 저장하는 기능

### 6.2 신경망의 추론 처리

MNIST 데이터셋을 가지고 추론을 수행하는 신경망을 구현해본다. 이 신경망은 입력층 뉴런을 784개, 출력층 뉴런을 10개로 구성한다.

입력층 뉴련이 784개인 이유: 이미지 크기가 28 * 28 = 784이기 때문이다.

출력층 뉴런이 10개인 이유: 이 문제가 0에서 9까지의 숫자를 구분하는 문제이기 때문이다.

은닉층은 두개로 구성하며, 첫 번째 은닉층에 50개의 뉴런을, 두 번째 은닉층에는 100개의 뉴런을 배치한다.

**신경망 구현**

```python
def get_data():
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

def init_network():
    with open('sample_weight.pkl', 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    
    return y
```

**정규화(normalization)**: 데이터를 특정 범위로 변환하는 처리, 위의 예에서 0~255 범위인 각 픽셀 값을 0~1 범위로 변환한다.

**전처리(pre-processing)**: 신경망의 입력 데이터에 특정 변환을 가하는 것을 의미한다.

현업에서는 신경망에 전처리를 활발히 사용한다. 예로 전체 평균과 표준편차를 이용해 데이터들이 0을 중심으로 분포하도록 이동하거나 데이터의 확산번위를 제한하는 정규화를 수행한다. 그 외에도 전체 데이터를 균일하게 분포시키는 데이터 백색화등도 있다.

### 6.3 배치 처리

데이터를 배치로 처리함으로써 효율적이고 빠르게 처리할 수 있다.

배치 처리 구현에 앞서 구현한 신경망 각 층의 가중치 형상을 출력해본다.

![img](https://blog.kakaocdn.net/dn/zsNqd/btqIqnehBwR/4EHADE5re6N6ZWvQQFIZI0/img.png)

이 결과에서 다차원 배열의 대응하는 차원의 원소수가 일치함을 확인 할 수 있다.

이미지 **여러 장을 한꺼번에** 입력하는 경우를 생각해본다.

이미지 100개를 묶어 predict() 함수에 한번에 넘긴다. x의 형상을 100 * 784로 바꿔서 100장 분량의 데이터를 하나의 입력 데이터로 표현하면 된다.

![img](https://blog.kakaocdn.net/dn/bOXWRm/btqIggOodRf/lnT8vNpyiCCTOSZK93N0XK/img.png)

​																						배치 처리를 위한 배열들의 형상 추이

입력 데이터의 형상은 100*784, 출력 데이터는 100 * 10이 된다.

이처럼 하나로 묶은 입력 데이터를 **배치(batch)**라고 한다.

배치 처리는 컴퓨터로 계산할 때 큰 이점이 있다.

1. 수치 계산 라이브러리 대부분이 큰 배열을 효율적으로 처리할 수 있도록 고도로 최적화되어 있기 때문이다.
2. 커다란 신경망에서는 데이터 전송이 병목으로 작용하는 경우가 자주 있는 데, 배치 처리를 함으로써 버스에 주는 부하를 줄인다.

즉, 배치 처리를 수행하여 큰 배열로 이뤄진 계산을 하게 되는데, 컴퓨터에서는 큰 배열을 한꺼번에 계산하는 것이 분할된 작은 배열을 여러번 계산하는 것보다 빠르다.

**배치 처리 구현**

```python
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_cnt = 0
for i in range(0, len(x), batch_size):
    x_batch = x[i : i + batch_size]
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis=1)
    accuracy_cnt += np.sum(p == t[i : i+batch_size])
    
print('Accuracy: ' + str(float(accuracy_cnt) / len(x)))
```

데이터를 배치로 처리함으로써 효율적으로 처리할 수 있다.



## 7. 정리

퍼셉트론과 신경망의 차이는 활성화 함수에 큰 차이가 있다. 신경망에서는 매끄럽게 변화하는 시그모이드 함수를 퍼셉트론에서는 계단 함수를 활성화 함수로 사용했다. 이 차이가 신경망 학습에서 매우 중요하다.







