# 신경망 - Neural Network

​	**신경망**은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습할 수 있습니다.



## 1. 퍼셉트론에서 신경망으로

### 	1.1 신경망의 예

​			신경망을 그림으로 나타내면 다음과 같이 됩니다. 입력층, 은닉층, 출력층으로 나뉘어져 있습니다.

![img](https://blog.kakaocdn.net/dn/QfyS1/btqH7cdVOs5/GhDWPa5tGZOTjsEK5FXc9K/img.png)

​																										 신경망의 예



### 1.2 활성화(activation) 함수의 등장

**활성화 함수**는 h(x)와 같이 입력 신호의 총합을 출력 신호로 변환하는 함수입니다. 활성화 함수는 입력 신호의 총합이 활성화를 일으키는지를 정하는 역할을 합니다.

![img](https://blog.kakaocdn.net/dn/cnJM8v/btqIas7Lhjl/xYCKD7Op78O3Xkx2sD9rBK/img.png)

​																							활성화 함수의 처리 과정

a는 가중치가 달린 입력신호와 편향의 총합을 계산합니다.

a를 함수 h()에 넣어 y를 출력합니다.



## 2. 활성화 함수

### 	2.1 시그모이드 함수

![img](https://blog.kakaocdn.net/dn/pzsVS/btqH7azGbTz/aaT7dldwXcOh2x55WdHOI1/img.png)



신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 그 변환된 신호를 다음 뉴런에 전달한다. 퍼셉트론과 신경망의 주된 차이는 이 활성화 함수 뿐이다.

### 	2.2 계단 함수 구현하기

- 계단 함수는 입력이 0을 넘으면 1을 출력하고, 그 외에는 0을 출력하는 함수		

```python
def step_function(x):
    y = x > 0
    return y.astype(np.int)
```



### 2.3 계단 함수의 그래프

![img](https://blog.kakaocdn.net/dn/bd3OjQ/btqIhtyFLC4/LYt9tAVVjd7qhX9dgkcpk1/img.png)

### 2.4 시그모이드 함수 구현하기

```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```

![img](https://blog.kakaocdn.net/dn/V6SaP/btqH8dwOW39/j2wDE9wUD5ydxKNNEbe2u0/img.png)

### 2.5 비선형 함수

**신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다**

이유는 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문이다.

예를 들어 선형함수인 h(x) = cx 활성화 함수를 사용한 3층 네트워크를 이용한다면, 식은  y(x) = c * c * c * x 처럼 곱셈을 세번 수행하지만, 실은 y(x) = ax와 똑같은 식이 된다. 따라서 층을 쌓은 이점을 얻고 싶다면 비선형 함수를 사용해야 한다.



### 2.7 ReLU 함수

ReLU는 입력이 0을 넘으면 그 입력 그대로 출력하고 0이하면 0을 출력하는 함수입니다.

![img](https://blog.kakaocdn.net/dn/etZ7S1/btqH50YR9m7/ttYnwjJKPkWMxqruX9tQS0/img.png)

​																									ReLU 함수의 그래프



수식으로는 다음과 같이 나타낼 수 있습니다.

![img](https://blog.kakaocdn.net/dn/9tPKf/btqIhsmgqAp/Xrbl0AKeA6AOZxry4rqPIK/img.png)

​																									ReLU함수 수식



ReLU 함수 구현

```python
def relu(x):
    return np.maximum(0, x)
```



## 3. 다차원 배열의 계산

넘파이 다차원 배열을 사용한 계산법을 숙달하면 신경망을 효율적으로 구현할 수 있다.

### 3.1 다차원 배열

​	**1차원 배열**

```python
import numpy as np
A = np.array([1,2,3,4])
```

​	**2차원 배열 **

```python
import numpy as np
B = np.array([[1,2],[3,4],[5,6]])
```



### 3.2 행렬의 내적(행렬 곱)

​	**행렬의 내적 구현하기**

```python
A = np.array([[1,2],[3,4]])
B = np.array([[5,6],[7,8]])

np.dot(A,B) #result : [[19,22],[43,50]]
```

다차원 배열을 곱하려면 두 행렬의 대응하는 차원의 원소 수를 일치시켜야 한다.

![img](https://blog.kakaocdn.net/dn/bqYHWv/btqIh0jhfty/RGnPJZynMTJ7YPGHjvq7F0/img.png)

### 3.3 신경망의 내적

​	넘파이 행렬을 이용한 신경망 구현

![img](https://blog.kakaocdn.net/dn/lVQUm/btqIje2V78C/k3yZ3DuTAHzdJstEd7892K/img.png)

이 신경망은 편향과 활성화 함수를 생략하고 가중치만 갖는다.

X와 W의 대응하는 차원의 원소수가 같아야 한다.

```python
X = np.array( [1, 2] )
X.shape # (2,)

W = np.array( [ [1, 3, 5], [2, 4, 6] ] )

Y = np.dot(X, W)
print(Y)
# [ 5, 11, 17 ]
```



## 4. 3층 신경망 구현하기

3층 신경망에서 수행되는 입력부터 출력까지의 처리(순방향 처리)를 구현

![img](https://blog.kakaocdn.net/dn/ceTcLY/btqIdst2l5K/1mIrucyKybXKKozk7R3mq0/img.png)

### 4.1 표기법 설명

![img](https://blog.kakaocdn.net/dn/bIMJUQ/btqH51cRoMa/afhti91x8oKB8lTjW93rnk/img.png)

​																										중요한 표기

### 4.2 각 층의 신호 전달 구현하기

입력층에서 1층의 첫번째 뉴런으로 가는 신호를 보자.

![img](https://blog.kakaocdn.net/dn/bfzbjj/btqIdr2Nk32/sKbLbFSeumzUGsmTV69y5K/img.png)

편향을 뜻하는 뉴런이 추가되었다. 편향은 오른쪽 아래 인덱스가 하나밖에 없다는 사실을 주의

지금까지 확인한 것을 반영하여 수식으로 나타내보겠습니다.
$$
a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}
$$
여기서 행렬의 내적을 이용하면 1층의 '가중치 부분'을 다음 식처럼 간소화 할 수 있습니다.
$$
A^{(1)} = XW^{(1)} + B^{(1)}
$$
이 때 행렬 A,X,B,W은 각각 다음과 같다.

![img](https://blog.kakaocdn.net/dn/d7S3B4/btqH8bzwl4r/5wj1IYhxsH6thgdyfTLpx1/img.png)

넘파이를 이용하여 위 식을 구현해보았습니다.

```python
X = np.array([1.0,0.5])
W1 = np.array([0.1,0.3,0.5],[0.2,0.4,0.6])
B1 = np.array([0.1,0.2,0.3])

A1 = np.dot(X,W) + B1
```

이어서 1층의 활성화 함수에서의 처리를 살펴본다. 이 활성화 함수의 처리를 그림으로 나타내면 다음과 같다.

![img](https://blog.kakaocdn.net/dn/yYuCi/btqIlu5BH3g/QpIhLP5nsumLwCWwefujH0/img.png)

​																							입력층에서 1층으로의 신호 전달

그림과 같이 은닉층에서 가중치 합(가중 신호와 편향의 총합)을 a로 표기하고 활성화 함수 h()로 변환된 신호를 z로 표기합니다. 활성화 함수는 시그모이드를 사용.

```python
Z1 = sigmoid(A1)
print(A1) # [0.3, 0.7, 1.1]
print(Z1) # [0.57444252. 0.66818777, 0.75026011]
```

이어서 1층에서 2층으로 가는 과정과 구현

![img](https://blog.kakaocdn.net/dn/baJUqH/btqIl9mGAzR/m16buWlb1WW7jteed4dcS0/img.png)

​															1층에서 2층으로의 신호 전달

```python
W2 = np.array( [ [0.1, 0.4], [0.2, 0.5], [0.3, 0.6] ] )
B2 = np.array( [0.1, 0.2])

A2 = np.dot(Z1, W2) + B2
Z2 = sigmoid(A2)
```

이처럼 넘파이 배열을 사용하면 층 사이의 신호 전달을 쉽게 구현할 수 있다.

마지막으로 2층에서 출력층으로의 신호 전달입니다.

은닉층과 다른 점은 활성화 함수만 다르다.

![img](https://blog.kakaocdn.net/dn/dpCmlZ/btqIdsHRnV9/dwkvd64jAivZzg0gKVlTPk/img.png)

출력층 활성화 함수는 항등 함수를 이용한다.

```python
def identity_function(x): # 출력층 활성화 함수는 함등함수를 이용했습니다.
    return x

W3 = np.array( [ [0.1, 0.3], [0.2, 0.4] ] )
B3 = np.array( [0.1, 0.2] )

A3 = np.dot(Z2, W3) + B3
Y = identity_function(A3) # 혹은 Y = A3
```

### 4.3 구현 정리

지금까지 구현을 정리

```python
def init_network(): # 가중치와 편향을 초기화하고 이들을 딕셔너리 변수인 network에 저장합니다.
    network = {}
    network['W1'] = np.array( [ [0.1, 0.3, 0.5], [0.2, 0.4, 0.6] ] )
    networt['b1'] = np.array( [0.1, 0.2, 0.3] )
    network['W2'] = np.array( [ [0.1, 0.4]. [0.2, 0.5], [0.3, 0.6] ] )
    network['b2'] = np.array( [0.1, 0.2] )
    network['W3'] = np.array( [ [0.1, 0.3], [0.2, 0.4] ] )
    network['b3'] = np.array( [0.1, 0.2] )
    
    return network
    

def forward(network, x): # 입력 신호를 출력으로 변환하는 처리 과정을 구현합니다.
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = identity_fuction(a3) # identity_fuction은 위에서 정의한 출력층 항등 함수를 의미합니다.
    
    return y


network = init_network()
x = np.array( [1.0, 0.5] )
y = forword(network, x)
print(y) # [0.31682708 0.69627909]
```

