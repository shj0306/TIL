# 신경망 학습 (1) - 손실 함수와 수치 미분

> 손실 함수는 신경망이 학습할 수 있도록 해주는 지표다. 이 손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 학습의 목표가 된다.

> 경사법 : 손실 함수의 값을 작게 만드는 기법으로 함수의 기울기를 활용한다.



### 1. 데이터에서 학습한다.

신경망의 특징은 데이터를 보고 학습할 수 있다. 데이터에서 학습한다는 것은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻.



### 1.1 데이터 주도 학습

기계학습이란 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것을 의미한다. 주어진 문제를 해결하려 할 때 사람, 기계학습, 신경망(딥러닝) 세 종류의 접근번에 대해 알아보자.

![img](https://blog.kakaocdn.net/dn/cfjytj/btqItaeAGEZ/s3AVGRQVZ6UkP2E5fnrHU0/img.png)

​																	  손글씨 숫자 '5'의 예: 사람마다 자신만의 필체가 있다.



딥러닝을 종단간 기계학습이라고도 한다. 여기서 종단간은 처음부터 끝까지라는 의미로, 데이터에서 목표한 결과를 사람의 개입없이 얻는다는 뜻을 담고 있다.



### 1.2 훈련 데이터와 시험 데이터

기계학습 문제는 데이터를 train data와 test data로 나눠 학습과 테스트를 수행한다.

우선 훈련 데이터만 사용해 학습하면서 최적의 매개변수를 찾아낸다. 그 다음 시험 데이터를 사용해 앞서 훈련한 모델의 능력을 평가하는 것이다.

## 2. 손실함수

신경망은 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다. 이 지표를 손실 함수라고 한다. 

손실함수는 일반적으로 **평균 제곱 오차**와 **교차 엔트로피 오차**를 사용한다.

### 2.1 평균 제곱 오차

가장 많이 쓰이는 손실 함수는 **평균 제곱 오차(MSE)**이다. 평균 제곱 오차는 수식으로 다음과 같다.

![img](https://blog.kakaocdn.net/dn/bhDvfS/btqIhY02ZSs/9okZjGHlrDoqLZLIS4ylUK/img.png)

여기서 yk는 신경망의 출력, tk는 정답 레이블, k는 데이터 차원수를 나타낸다.

```python
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 소프트맥스 함수의 출력
t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]                       # 정답을 가르키는 위치의 원소
```

평균 제곱 오차는 각 원소의 출력과 정답 레이블의 차를 제곱한 후, 총합을 구하고 데이터 수로 나눈다.

#### 평균 제곱 오차 구현

```python
def mean_squared_error(y, t):
    return 0.5 * np.sum((y-t)**2) #y와t는 넘파이 배열

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 정답은 2
y1 = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 2일 확률이 가장 높다고 추정함 (0.6)

print( mean_squared_error(np.array(y1), np.array(t)) )
>>> 0.0975000000031


y2 = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 7일 확률이 가장 높다고 추정함 (0.6)
print( mean_squared_error(np.array(y2), np.array(t)) )
>>> 0.597500000003
```

평균 제곱 오차를 기준으로는 첫 번째 추정결과가 정답에 더 가까울 것으로 판단할 수 있다



### 2.2 교차 엔트로피 오차

![img](https://blog.kakaocdn.net/dn/EtRYr/btqIsMd0Ly5/BLgDKHTLgyk0wibaOCydbk/img.png)

yk는 신경망의 출력, tk는 정답 레이블. 또 tk는 정답에 해당하는 인덱스의 원소만 1이고 나머지는 0이다. 그래서 위 식은 실질적으로 정답일 때의 추정(tk가 1일 때의 yk)의 자연로그를 계산하는 식이 된다. (정답이 아닌 나머지 모두 tk가 0이므로 logyk와 곱해도 0이 되어 결과에 영향을 주지 않는다.)



**교차 엔트로피 오차 구현**

```python
def cross_entropy_error(y, t):
    delta = 1e-7
    return -np.sum(t * np.log(y+delta))

t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]                       # 정답은 2
y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 신경망이 2로 추정

print( cross_entropy_error(np.array(y), np.array(t) )
>>> 0.5108254709933802


y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 신경망이 7로 추정

print( cross_entropy_error(np.array(y), np.array(t) )
>>> 2.3025840929945458
```

앞에 있는 평균 제곱 오차의 판단과 일치한다.



### 2.3 미니배치 학습

미니배치: 훈련 데이터로부터 일부만 골라 학습을 수행한다. 예를 들어 6만장의 훈련 데이터 중에서 100장을 무작위로 뽑는다.

미니배치 학습: 뽑아낸 100장만을 사용하여 학습하는 것을 의미한다.

![img](https://blog.kakaocdn.net/dn/oZpJM/btqIpwXnlmZ/RXu151kOTZOrckMtk6Kfe0/img.png)

데이터가 N개라면 tnk는 n번째 데이터의 k번째 값을 의미한다. (ynk는 신경망의 출력, tnk는 정답 레이블) 이 식은 데이ㅓ 하나에 대한 손실 함수의 식을 단순히 N개의 데이터로 확장했을 뿐이다.

다만, 마지막에 N으로 나누어 정규화하고 있다. N으로 나눔으로써 평균 손실 함수를 구할 수 있다.

이렇게 평균을 구해 사용하면 훈련 데이터 개수와 관계없이 언제든 통일된 지표를 얻을 수 있다.



**미니배치 구현**

```python
import sys, os
sys.path.append(os.pardir)
import numpy as np
from dataset.mnist import load_mnist

(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000, 10)

train_size = x_train.shape[0]   # 60000 의미
batch_size = 10
batch_mask = np.random.choice(train_size, batch_size) # 60000개 중 10개 뽑기
# np.random.choice를 이용해 무작위로 원하는 개수만 꺼낼 수 있습니다.

x_batch = x_train[batch_mask]
t_batch = t_train[batch_mask]
```



### 2.4 (배치용) 교차 엔트로피 오차 구현

t가 원-핫 인코딩일 때

```python
def cross_entropy_error(y, t):
    if y.ndim == 1:					# 배치사이즈가 1일때
        t = t.reshape(1, t.size)    # 1-D 로 변환, t.size는 t의 요소 갯수를 의미
        y = y.reshape(1, y.size)    # 1-D 로 변환
       
    batch_size = y.shape[0]
    return -np.sum(t * np.log(y)) / batch_size # 원핫인코딩일 경우
                                               # t가 0일때는 교차 엔트로피 오차도 0이므로 
                                               # 무시해도 좋다는 것이 핵심
```

t가 원-핫 인코딩이 아닐 때

```python
def cross_entropy_error(y,t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    batch_size = y.shape[0]
    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size
```

> 원-핫 인코딩 시 t * np.log(y)

> 원-핫 인코딩이 아닐 경우 np.log( y[ np.arange(batch_size), t] )로 구현합니다.

**np.log(y[np.arange(batch_size), t]) 설명**

> np.arange(batch_size)는 0부터 batch_size-1까지 배열을 생성한다.

> batch_size가 5면 [0,1,2,3,4] 넘파이 배열을 생성한다.

> t에는 레이블이 [2,7,0,9,4]와 같이 저장되어 있으므로 y[np.arange(batch_size), t]는 각 데이터의 정답 레이블에 해당하는 신경망의 출력을 추출한다.

> 따라서 [y[0,2], y[1,7], y[2,0], y[3,9], y[4,4]]인 넘파이 배열을 생성한다.



### 2.5 왜 손실 함수를 설정하는가?

신경망 학습에서는 **손실함수의 미분**를 계산하고, 미분 값이 0이 되는 쪽으로 매개변수를 갱신해준다.

정확도를 지표로 삼아서 안되는 이유는 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문이다. 정확도는 매개변수의 미소한 변화에는 거의 반응하지 않고, 반응이 있어도 값들이 불연속적으로 변화한다.

이는 계단 함수를 활성화 함수로 사용하지 않는 이유와 같다.

![img](https://blog.kakaocdn.net/dn/bGEjOM/btqInrIZnLt/ZkKWF0OchZn8oiFFxnQIFk/img.png)



계단 함수의 미분은 대부분의 장소에서 기울기가 0이지만, 시그모이드 함수의 미분은 출력이 연속적으로 변하고, 곡선의 기울기도 연속적으로 변한다. 즉 시그모이드 함수의 미분은 어느 장소라도 0이 되지는 않는다.

이는 신경망 학습에서 중요한 성질로, 기울기가 0이 되지 않기 때문에 올바르게 학습이 가능하다.



## 3. 수치 미분 (Numerical differentiation)

>수치 미분이란?
>
>해석적 미분은 우리가 수학 시간에 배운 오차를 포함하지 않는 진정한 미분 값을 구해준다.
>
>**수치 미분**은 이를 근사치로 계산하는 방법이다. 따라서 오차가 포함된다. 이와 관련해 수치해석학은 해석학 문제에서 수치적인 근삿값을 구하는 알고리즘을 연구하는 학문이다.



### 3.1 미분

![img](https://blog.kakaocdn.net/dn/cfsXKF/btqInM64oCt/N60I1vrWVirmuWUjZEw7O0/img.png)

x의 작은 변화가 함수 f(x)를 얼마나 변화시키는 가를 의미한다.

**미분 구현**

```python
# 나쁜 구현 예
def numerical_diff(f, x):
    h = 10e - 50
    
    return (f(x + h) - f(x)) / h
```

개선해야 할 점이 2개가 있다.

1. **반올림 오차 문제**

   반올림 오차는 작은 값이 생략되어 최종 계산 결과에 오차가 생기게 된다. h를 0으로 무한히 가깝게 하기 위해 1e-50이라는 작은 값을 이용했다. 너무 작은 값을 이용하면 컴퓨터로 계산하는 데 문제가 생긴다.

2. **함수 f의 차분 문제** (차분은 임의의 두 점에서 함수 값들의 차이를 의미한다.)

   앞의 구현에서 x+h와 x사이의 함수 f의 차분을 계산하고 있지만, 이 계산에는 오차가 있다.  진정한 미분은 x 위의 함수의 기울기에 해당하지만, 이 구현엣 미분은 x+h와 x사이의 기울기에 해당한다. 이 차이는 h를 무한히 0으로 좁히는 것이 불가능해 생기는 한계다.

   - 오차를 줄이기 위해 **중심 차분** 혹은 **중앙 차분**을 이용한다.

   > **중심 차분, 중앙 차분이란?**

   > (x+h)와 (x-h)일 때의 함수 f의 차분을 계산하는 방법을 의미한다. 이 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 중심 차분 혹은 중앙 차분이라 한다.

   ![img](https://blog.kakaocdn.net/dn/c1L8b3/btqIvFlwIxj/IztzTt44GGZKD2GMuphFV1/img.png)

**두 개선점을 적용해 수치 미분을 구현**

```python
def numerical_diff(f, x):
    h = 1e-4
    return (f(x+h) - f(x-h) / (2 * h)) # 중앙 차분 이용
```



### 3.2 편미분

편미분은 변수가 여럿인 함수에 대한 미분을 의미한다.
$$
f(x_0,x_1) = x_0^2 + x_1^2
$$
이 식은 변수가 2개다.

```python
def function_2(x):
    return np.sum(x ** 2)
```

![img](https://blog.kakaocdn.net/dn/4hvQj/btqIs8nVORh/Mxlu9gwdnJ6uNhwT9HrS10/img.png)
$$
f(x_0,x_1) = x_0^2 + x_1^2
$$
**편미분 구현**

```python
x0 = 3
x1 = 4

def function_tmp1(x0):				# x0에 대한 편미분
    return x0 * x0 + 4.0**2.0

numerical_diff(function_tmp1, 3.0)
>>> 6.000000378


def function_tmp2(x1):               # x1에 대한 편미분
    return 3.0 ** 2.0 + x1 ** x1
    
numerical_diff(function_tmp2, 4.0)
>>> 7.99999999119
```

다른 변수는 값을 고정하고 목표 변수 하나에 초점을 맞춘다. 목표 변수를 제외한 나머지를 특정 값에 고정하기 위해 새로운 함수를 정의한다. 그리고 새로 정의한 함수에 그동안 사용한 수치 미분 함수를 적용해 편미분을 구한다.

