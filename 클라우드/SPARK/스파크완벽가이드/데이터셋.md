# 데이터셋

- 데이터셋은 자바 가상 머신(JVM)을 사용하는 언어인 스칼라와 자바에서만 사용할 수 있습니다. 
- 스칼라에서는 스키마가 정의된 케이스 클래스 객체를 사용해 데이터셋을 정의하고, 자바에서는 자바 빈 객체를 사용해 데이터셋을 정의합니다.
- 스파크는 StringType, BigIntType, StructType과 같은 다양한 데이터 타입을 제공합니다.
- 도메인별 특정 객체를 효과적으로 지원하기 위해 인코더라 부르는 특수 개념이 필요합니다. 인코더는 도메인별 특정 객체 T를 스파크의 내부 데이터 타입으로 매핑하는 시스템을 의미합니다.
- 인코더는 런타임 환경에서 객체를 바이너리 구조로 직렬화하는 코드를 생성하도록 스파크에 지시합니다. 데이터프레임이나 표준 구조적 API를 사용한다면 Row 타입을 직렬화된 바이너리 구조로 변환합니다.
-  도메인에 특화된 객체를 만들어 사용하려면 스칼라의 케이스 클래스 또는 자바의 자바빈 형태로 사용자 정의 데이터 타입을 정의해야 합니다.
- 데이터셋 API를 사용하면 스파크는 데이터셋에 접근할 때마다 Row 포맷이 아닌 사용자 정의 데이터 타입을 변환합니다. 사용자 정의 데이터 타입을 사용하면 성능은 나빠지지만 더 많은 유연성을 제공해 줍니다.
- **프로그래밍 언어를 전환하는 것이 사용자 정의 데이터 타입을 사용하는 것보다 훨씬 느립니다**



## 데이터셋 사용 시기

### 데이터셋을 사용해야 하는 이유

1. 데이터프레임 기능만으로는 수행할 연산을 표현할 수 없는 경우
2. 성능 저하를 감수하더라도 타입 안정성을 가진 데이터 타입을 사용하고 싶은 경우

- 구조적 API를 사용해 표현할 수 없는 작업들이 있습니다. 복잡한 비즈니스 로직을 SQL이나 데이터프레임 대신 단일 함수로 인코딩해야 하는 경우가 있습니다. 이런 경우는 데이터셋을 사용하는 것이 적합합니다.

- 데이터셋 API는 타입 안정성이 있습니다. 데이터 타입이 유효하지 않은 작업은 런타임이 아닌 컴파일 타임에 오류가 발생합니다. 정확도와 방어적 코드를 중요시 한다면 성능을 조금 희생하더라도 데이터셋을 사용하는 것이 좋습니다.
- 단일 노드의 워크로드와 스파크 워크로드에서 전체 로우에 대한 다양한 트랜스포메이션을 재사용하려면 데이터셋을 사용하는 것이 좋습니다. 데이터 셋을 사용하는 장점 중 하나는 로컬과 분산 환경의 워크로드에서 재사용할 수 있다는 겁니다. 
- 더 적합한 워크로드를 만들기 위해 데이터프레임과 데이터셋을 동시에 사용해야 할 때가 있습니다. 이러한 방식은 대량의 데이터프레임 기반의 ETL 트랜스포메이션의 마지막 단계에서 사용할 수 있습니다.

## 데이터셋 생성

### 자바 : Encoders

- 데이터 타입 클래스를 정의한 다음, 데이터프레임 (DataSet<Row> 타입의)에 지정해 인코딩할 수 있습니다.

### 스칼라 : 케이스 클래스

- 스칼라에서 데이터셋을 생성하려면 case class 구문을 사용해 데이터 타입을 정의해야 합니다.

#### 특징

- 불변성
- 패턴 매칭으로 분해 가능
- 참조값 대신 클래스 구조를 기반으로 비교
- 사용하기 쉽고 다루기 편함

```scala
case class Flight(dest_country_name : String, origin_country_name : String, count : BigInt)
```

```scala
val flightsDF = spark.read.parquet("/FileStore/tables/2010-summary.parquet")
val flights = flightsDF.as[Flight]
```

- Flight 데이터 타입은 스키마만 정의되어 있을 뿐 아무런 메서듣도 정의되어 있지 않습니다.
- 데이터프레임에 as 메서드를 사용해 Flight 데이터 타입으로 변환합니다.

## 액션

- 데이터셋이나 데이터프레임에 collect, take, count 같은 액션을 적용할 수 있습니다.
- 케이스 클래스에 실제로 접근할 때 어떠한 데이터 타입도 필요하지 않습니다. 케이스 클래스의 속성명을 지정하면 속성에 맞는 값과 타입을 모두 반환합니다.

## 트랜스포메이션

- 데이터셋의 트랜스포메이션은 데이터프레임과 동일합니다. 

### 필터링

- Flight 클래스를 파라미터로 사용해 불리언 값을 반환하는 함수를 만듭니다. 불리언 값은 출발지와 도착지가 동일한 지 아닌지를 나타냅니다. 이 함수는 사용자 정의 함수가 아닌 일반 함수입니다.

  ```scala
  def originIsDestination(flight_row : Flight) : Boolean = {
    return flight_row.origin_country_name == flight_row.dest_country_name
  }
  ```

- 위에서 정의한 함수를 filter 메서드에 적용해 각 행이 true를 반환하는 지 평가하고 데이터셋을 필터링할 수 있습니다.

  ```scala
  flights.filter(flight_row => originIsDestination(flight_row)).first()
  
  //Flight = Flight(United States,United States,348113)
  ```

### 매핑

- 데이터프레임의 매칭 작업을 수행하는 것은 데이터셋의 select 메서드를 사용하는 것과 같습니다.

- 예제) 목적지 컬럼을 추출

  ```scala
  val destinations = flights.map(f => f.dest_country_name)
  ```

- 최종적으로 String 데이터 타입의 데이터셋을 반환합니다. 스파크는 결과로 반환할 JVM 데이터 타입을 알고 있기 때문에 컴파일 타임에 데이터 타입의 유효성을 검사 할 수 있습니다.

- 데이터프레임을 사용해도 대다수 매핑 작업을 수행할 수 있습니다.

## 조인

- 데이터프레임에서와 마찬가지로 데이터셋에도 동일하게 적용됩니다. 하지만 데이터셋은 joinWith처럼 정교한 메서드를 제공합니다. joinWith 메서드는 co-group과 거의 유사하며 데이터셋 안쪽에 다른 두 개의 중첩된 데이터셋으로 구성됩니다. 각 컬럼은 단일 데이터셋이므로 데이터셋 객체를 컬럼처럼 다룰 수 있습니다. 그렇기 때문에 조인 수행 시 더 많은 정보를 유지할 수 있고, 고급 맵이나 필터처럼 정교하게 데이터를 다룰 수 있습니다.

- 예제) joinWith 메서드를 적용하기 위해 가짜 항공운항 메타데이터를 생성합니다.

  ```scala
  case class FlightMetadata(count: BigInt, randomData:BigInt)
  
  val flightsMeta = spark.range(500).map(x => (x, scala.util.Random.nextLong))
                          .withColumnRenamed("_1", "count").withColumnRenamed("_2", "randomData")
                          .as[FlightMetadata]
  
  val flights2 = flights.joinWith(flightsMeta, flights.col("count") === flightsMeta.col("count"))
  ```

  최종적으로 로우는 Flight와 FlightMetadata로 이루어진 일종의 키~값 형태의 데이터셋을 반환합니다.

- 일반 조인 역시 잘 동작합니다. 하지만 데이터프레임을 반환하므로 JVM 데이터 타입 정보를 잃게 됩니다.

  ```scala
  val flights2 = flights.join(flightsMeta, Seq("count"))
  ```

## 그룹화와 집계

- 그룹화와 집계는 동일한 기본 표준을 따르기 때문에 groupBy, rollup, cube메서드를 사용할 수 있지만, 데이터프레임을 반환하기 때문에 데이터 타입 정보를 잃게 됩니다.

- groupByKey 메서드는 데이터셋의 특정 키를 기준으로 그룹화하고 형식화된 데이터셋을 반환합니다. 하지만 이 함수는 컬럼명 대신 함수를 파라미터로 사용해야 합니다.

- 예제) groupByKey 메서드를 사용해 데이터프레임에 새로운 컬럼을 추가한 다음 그룹화를 수행

  ```scala
  flights.groupByKey(x => x.dest_country_name).count().explain
  
  /*
  == Physical Plan ==
  AdaptiveSparkPlan isFinalPlan=false
  +- HashAggregate(keys=[value#174], functions=[finalmerge_count(merge count#185L) AS count(1)#175L])
     +- Exchange hashpartitioning(value#174, 200), ENSURE_REQUIREMENTS, [id=#395]
        +- HashAggregate(keys=[value#174], functions=[partial_count(1) AS count#185L])
           +- Project [value#174]
              +- AppendColumns ), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true, false, true) AS value#174]
  */
  ```

  데이터셋의 키를 이용해 그룹화를 수행한 다음 결과를 키-값 형태로 함수에 전달해 원시 객체 형태로 그룹화된 데이터를 다룰 수 있습니다.

  ```scala
  def grpSum(countryName : String, values: Iterator[Flight]) = {
      values.dropWhile(_.count < 5).map(x => (countryName, x))
  }
  
  flights.groupByKey(x => x.dest_country_name).flatMapGroups(grpSum).show(5)
  ```

  ```scala
  def grpSum2(f:Flight):Integer = {
    1
  }
  flights.groupByKey(x => x.dest_country_name).mapValues(grpSum2).count().take(5)
  ```

- 예제) 새로운 처리 방법을 생성해 그룹을 축소하는 방법

  ```scala
  def sum2(left:Flight, right:Flight) = {
    Flight(left.dest_country_name, null, left.count + right.count)
  }
  
  flights.groupByKey(x => x.dest_country_name).reduceGroups((l,r)=>sum2(l,r)).take(5)
  ```

  groupByKey 메서드는 동일한 결과를 반환하지만 데이터 스캔 직후에 집계를 수행하는 groupBy 메서드에 비해 더 비싼 처리란 것을 아래 결과를 통해 알 수 있습니다.

  ```scala
  flights.groupBy("dest_country_name").count().explain
  
  /*
  AdaptiveSparkPlan isFinalPlan=false
  +- HashAggregate(keys=[dest_country_name#54], functions=[finalmerge_count(merge count#293L) AS count(1)#288L])
     +- Exchange hashpartitioning(dest_country_name#54, 200), ENSURE_REQUIREMENTS, [id=#746]
        +- HashAggregate(keys=[dest_country_name#54], functions=[partial_count(1) AS count#293L])
           +- FileScan parquet [DEST_COUNTRY_NAME#54] Batched: true, DataFilters: [], Format: Parquet, Location: InMemo
  */
  ```

  그러므로 사용자가 정의한 인코딩으로 세밀한 처리가 필요하거나 필요한 경우에만 데이터셋의 groupByKey 메서드를 사용해야 합니다. 데이터셋은 빅데이터 처리 파이프라인의 처음과 끝 작업에서 주로 사용합니다.

  