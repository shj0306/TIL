# 스파크 애플리케이션 개발하기

## 1. 스파크 애플리케이션 작성하기

스파크 애플리케이션은 스파크 클러스터와 사용자 코드 두가지 조합으로 구성됩니다. 예제에서는 클러스터 모드를 로컬 모드로 설정하고, 사전에 정의된 애플리케이션을 사용자 코드로 사용합니다.

## 1-1. 스칼라 기반 앱

스파크 애플리케이션은 두 가지 자바 가상 머신 기반의 빌드 도구인 sbt나 아파치 메이븐을 이용해 빌드할 수 있습니다. 각각 장단점이 있지만 sbt를 사용하는 것이 더 쉽습니다. sbt는 sbt웹사이트에서 내려받을 수 있으며 설치와 사용법을 배울 수 있습니다. 메이븐 역시 사이트에서 설치할 수 있습니다.

스칼라 애플리케이션에 sbt 빌드 환경을 구성하려면 패키지 정보를 관리하기 위해 build.sbt 파일을 정의해야 합니다. 

### build.sbt 파일에 포함되어야 할 항목

- 프로젝트 메타데이터
- 라이브러리 의존성을 관리하는 장소
- 라이브러리에 포함된 의존성 정보

 #### 해당 책이 스파크 2버전이라 현재 에러가 발생 => 현재 해결방법 찾는 중



## 1-2. 파이썬 애플리케이션 작성하기

```python
from __future__ import print_function

if __name__ == '__main__':

    from pyspark.sql import SparkSession

    spark = SparkSession.builder \
        .master("local") \
        .appName("Word Count") \
        .config("spark.some.config.option", "some-value") \
        .getOrCreate()
        
    print(spark.range(5000).where("id > 500").selectExpr("sum(id)").collect())
```

스파크에는 빌드 개념이 없으며 pyspark 애플리케이션은 파이썬 스크립트에 지나지 않기 때문에 클러스터에서 스크립트를 실행하기만 하면 됩니다.

코드를 실행하려면 SparkSession을 생성하는 실행 가능한 스크립트 파일을 만들어야 합니다.

### 애플리케이션 실행하기

```shell
$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py
```

### 스파크 애플리케이션 테스트

### 1. 전략적 원칙

데이터 파이프라인과 애플리케이션에 대한 테스트 코드 개발은 실제 애플리케이션 개발만큼이나 중요합니다. 테스트 코드는 미래에 발생할 수 있는 데이터, 로직 그리고 결과 변화에 유연하게 대처할 수 있게 도와줍니다.

#### 입력 데이터에 대한 유연성

데이터 파이프라인은 다양한 유형의 입력 데이터에 유연하게 대처할 수 있어야 합니다. 비즈니스 요구사항이 변하게 되면 데이터도 변하게 됩니다. 따라서 스파크 애플리케이션과 파이프라인은 입력 데이터 중 일부가 변하더라도 유연하게 대처할 수 있어야 합니다. 따라서 입력 데이터로 인해 발생할 수 있는 다양한 예외 상황을 테스트하는 코드를 작성해야 합니다.

#### 비즈니스 로직 변경에 대한 유연성

파이프라인 내부 로직이 바뀔 수도 있습니다. 이는 원하는 결과를 얻을 수 있도록 실제와 유사한 데이터를 사용해 비즈니스 로직을 꼼꼼하게 테스트해야 함을 의미합니다. 이 유형의 테스트에서는 스파크 단위 테스트를 작성하지 않도록 조심해야 합니다. 대신 비즈니스 로직을 테스트해 복잡한 비즈니스 파이프라인이 의도한대로 동작하는 지 반드시 확인해야 합니다.

#### 결과의 유연성과 원자성

입력 데이터 및 비즈니스 로직의 테스트가 완료되었다면 결과를 의도한 대로 반환하는 지 확인해야 합니다. 즉 결과 데이터가 스키마에 맞는 적절한 형태로 반환될 수 있도록 제어해야 합니다. 대부분의 스파크 파이프라인은 다른 스파크 파이프라인의 입력으로 사용됩니다. 데이터를 소비한 다음 데이터가 얼마나 자주 갱신되는 지, 데이터가 완벽한지, 마지막에 데이터가 변경되진 않았는 지를 이해할 수 있도록 만들고 싶을 겁니다.

### 2. 테스트 코드 작성 시 고려사항

적절한 단위 테스트를 작성해 입력 데이터나 구조가 변경되어도 비즈니스 로직이 정상적으로 동작하는 지 확인해야 합니다. 단위 테스트를 하면 스키마가 변경되는 상황에 쉽게 대응할 수 있습니다.

#### SparkSession 관리하기

스파크 로컬 모드 덕분에 JUnit이나 ScalaTest 같은 단위 테스트용 프레임워크로 비교적 쉽게 스파크 코드를 테스트할 수 있습니다. 테스트 하네스(test harness)의 일부로 로컬 모드의 SparkSession을 만들어 사용하기만 하면 됩니다. 이런 테스트 방식이 잘 동작하려면 스파크 코드에서 의존성 주입 방식으로 SparkSession을 관리하도록 만들어야 합니다. 즉 SparkSession을 한번만 초기화하고 런타임 환경에서 함수와 클래스에 전달하는 방식을 사용하면 테스트 중에 SparkSession을 쉽게 교체할 수 있습니다.

#### 테스트 코드용 스파크 API 선정하기

스파크는 sql, dataframe, dataset 등 다양한 api를 제공합니다. 각 api는 사용자 애플리케이션의 유지 보수성과 테스트 용이성 측면에서 서로 다른 영향을 미칠 수 있습니다. 적합한 api는 사용자가 속한 팀과 팀에서 무엇을 필요로 하는 지에 따라 달라질 수 있습니다.

api 유형에 상관없이 각 함수의 입력과 출력 타입을 문서로 만드로 테스트해야 합니다. 타입 안정성 api를 사용하면 함수가 가지고 있는 최소한의 규약을 지켜야 하므로 다른 코드에서 재사용하기 쉽습니다. 모든 동적 데이터 타입 언어가 그렇듯 dataframe이나 sql을 사용할 때는 혼란을 없애기 위해 각 함수의 입력 타입과 출력 타입을 문서로 만들고 테스트하는 노력이 필요합니다. 저수준 rdd api는 정적 데이터 타입을 사용하지만 dataset api는 없는 파티셔닝 같은 저수준 api의 기능이 필요한 경우에만 사용합니다.

애플리케이션에 사용할 언어를 선택할 때도 비슷한 고려사항이 적용됩니다.

대규모 애플리케이션이나 저수준 api를 사용해 성능을 완전히 제어하려면 스칼라와 자바 같은 정적 데이터 타입의 언어를 사용하는 것을 추천합니다. 반면 파이썬이나 R은 각 언어가 제공하는 강력한 라이브러리를 활용하려는 경우에 사용하는 것이 좋습니다.

#### 데이터소스 연결하기

가능하면 테스트 코드에서는 운영 환경의 데이터소스에 접속하지 말아야 합니다. 그래야 데이터소스가 변경되더라도 고립된 환경에서 개발자가 쉽게 테스트 코드를 실행할 수 있습니다. 이런 환경을 구성하는 방법 중 하나로 비즈니스 로직을 가진 함수가 데이터소스에 직접 접근하지 않고 데이터프레임이나 데이터셋을 넘겨받게 만들 수 있습니다. 스파크의 구조적 API를 사용하는 경우 이름이 지정된 테이블을 이용해 문제를 해결할 수 있습니다.
