# 다양한 데이터 타입 다루기

## 스파크 데이터 타입으로 변환

- 데이터 타입 변환은 lit 함수를 사용합니다.

  ```scala
  import org.apache.spark.sql.functions.lit
  
  df.select(lit(5), lit("five"), lit(5.0))
  ```

## 불리언 데이터 타입

- 스칼라에서 ==, ===은 특별한 의미를 가집니다. 스파크에서 동등 여부를 판별해 필터링하면 일치를 나타내는 ===이나 불일치를 나타내는 =!=을 사용해야 합니다. not이나 equalTo를 사용할 수도 있습니다.

  ```scala
  import org.apache.spark.sql.functions.col
  
  // invoiceNo 값이 536365인 레코드 중 invoiceNo와 Description 컬럼 내용만 출력
  df.where(col("InvoiceNo") == 536365)
    .select("InvoiceNo", "Description")
    .show(5, false)
  ```

- 가장 명확한 방법은 문자열 표현식에 조건절을 명시하는 것입니다. 파이썬, 스칼라 모두에서 사용할 수 있습니다.

  ```scala
  df.where("InvoiceNo == 536365")
  df.where("InvoiceNo <> 536365")
  ```

- and, or 메소드를 사용해서 불리언 표현식을 여러 부분에 지정할 수 있습니다. 이 때 항상 모든 표현식을 and 메소드로 묶어서 차례대로 필터에 적용해야 합니다.

  ```scala
  val priceFilter = col("UnitPrice") > 600
  val descripFilter = col("Description").contains("POSTAGE")
  
  df.where(col("StockCode").isin("DOT")).where(priceFilter.or(descripFilter)).show()
  ```

  ```python
  from pyspark.sql.functions import instr
  # instr : 문자열에서 지정된 문자열을 검색해서 그 위치를 리턴하는 함수
  
  priceFilter = col("UnitPrice") > 600
  descripFilter = instr(df.Description, "POSTAGE") >= 1
  df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()
  ```

- 불리언 컬럼을 사용해서 데이터프레임을 필터링할 수 있습니다.

  ```scala
  val DOTCodeFilter = col("StockCode") === "DOT"
  val priceFilter = col("UnitPrice") > 600
  val descripFilter = col("Description").contains("POSTAGE")
  
  df.withColumn("isExpensive", DOTCodeFilter.and(priceFilter.or(descripFilter)))
    .where("isExpensive")
    .select("unitPrice", "isExpensive").show(5)
  ```

  ```python
  from pyspark.sql.functions import instr
  
  DOTCodeFilter = col("StockCode") == "DOT"
  priceFilter = col("UnitPrice") > 600
  descripFilter = instr(col("Description"), "POSTAGE") >= 1
  df.withColumn("isExpensive", DOTCodeFilter & (priceFilter | descripFilter)).where("isExpensive").select("unitPrice", "isExpensive").show(5)
  ```

- 필터를 표현할 때는 데이터프레임 인터페이스 방식보다 sql이 훨씬 쉽습니다.

  ```scala
  import org.apache.spark.sql.functions.{expr, not, col}
  
  // 두 코드는 같은 결과를 출력합니다.
  df.withColumn("isExpensive", not(col("UnitPrice").leq(250)))
    .filter("isExpensive")
    .select("Description", "UnitPrice").show(5)
  
  df.withColumn("isExpensive", expr("NOT UnitPrice <= 250"))
    .filter("isExpensive")
    .select("Description", "UnitPrice").show(5)
  ```

## 수치형 데이터 타입

- pow 함수

  ```scala
  import org.apache.spark.sql.functions.{expr, pow}
  
  val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
  df.select(expr("CustomerID"), fabricatedQuantity.alias("realQuantity")).show(2)
  ```

  ```scala
  // spark sql code
  df.selectExpr(
    "CustomerId",
    "(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity"
  )show(2)
  ```

- 반올림 round 함수, 내림 bround 함수

  ```scala
  import org.apache.spark.sql.functions.lit
  
  df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
  ```

- StatFunctions 패키지는 다양한 통계함수를 제공합니다. stat 속성을 사용해서 접근할 수 있으며 다양한 통곗값을 계산할 때 사용하는 데이터프레임 메소드입니다.

  - approxQuantile

    ```scala
    val colName = "UnitPrice"
    val quantileProbs = Array(0.5)
    val relError = 0.05
    
    // 데이터의 백분위수 계산
    df.stat.approxQuantile("UnitPrice", quantileProbs, relError)
    ```

  - crosstab / freqItems

    ```scala
    df.stat.crosstab("StockCode", "Quantity").show()
    df.stat.freqItems(Seq("StockCode", "Quantity")).show()
    ```

  - monotonically_increasing_id : 모든 로우에 고유 ID 값을 추가

    ```scala
    import org.apache.spark.sql.functions.monotonically_increasing_id
    df.select(monotonically_increasing_id()).show(2)
    ```

## 문자열 데이터 타입

- 문자열을 다루는 작업은 거의 모든 데이터 처리 과정에서 발생합니다. 로그 파일에 정규 표현식을 사용해 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등의 작업을 할 수 있습니다.

- 대/소문자 변환 작업

  - initcap 함수는 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경합니다.

    ```scala
    import org.apache.spark.sql.functions.{initcap}
    
    df.select(initcap(col("Description"))).show(2, false)
    ```

  - lower 함수는 문자열 전체를 소문자로 변경하거나, upper 함수를 사용해 문자열 전체를 대문자로 변경할 수 있습니다.

    ```scala
    import org.apache.spark.sql.functions.{lower,upper}
    
    var col_obj = col("Description")
    df.select(col_obj, lower(col_obj), upper(col_obj)).show(2)
    ```

  - 문자열 주변의 공백을 제거하거나 추가하는 작업 : lpad, ltrim, rpad, rtrim, trim 함수를 사용합니다.

    ```scala
    import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}
    
    df.select(
      ltrim(lit("    hello    ")).as("ltrim"),
      rtrim(lit("    hello    ")).as("rtrim"),
      trim(lit("    hello    ")).as("trim"),
      lpad(lit("hello"), 3, " ").as("lpad"),
      rpad(lit("hello"), 10, " ").as("rpad"),
    ).show(2)
    
    /*
    +---------+---------+-----+----+----------+
    |    ltrim|    rtrim| trim|lpad|      rpad|
    +---------+---------+-----+----+----------+
    |hello    |    hello|hello| hel|hello     |
    |hello    |    hello|hello| hel|hello     |
    +---------+---------+-----+----+----------+
    */
    ```

## 정규표현식

- 문자열의 존재 여부를 확인하거나 일치하는 모든 문자열을 치환할 때 보통 정규 표현식을 사용합니다. 정규 표현식을 사용해 문자열에서 값을 추출하거나 다른 값으로 치환하는 데 필요한 규칙 모음을 정의할 수 있습니다.

- 스파크는 정규 표현식을 위해 regexp_extract 함수와 regexp_replace 함수를 제공합니다.

  ```scala
  // regexp_replace 함수를 사용해 'description' 컬럼의 값을 'COLOR'로 치환해본다.
  
  import org.apache.spark.sql.functions.regexp_replace
  
  val simpleColors = Seq("black", "white", "red", "green", "blue")
  val regexString = simpleColors.map(_.toUpperCase).mkString("|")
  
  df.select(
      regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),
      col("Description")
  ).show(5)
  
  +--------------------+--------------------+
  |         color_clean|         Description|
  +--------------------+--------------------+
  |COLOR HANGING HEA...|WHITE HANGING HEA...|
  | COLOR METAL LANTERN| WHITE METAL LANTERN|
  |CREAM CUPID HEART...|CREAM CUPID HEART...|
  |KNITTED UNION FLA...|KNITTED UNION FLA...|
  |COLOR WOOLLY HOTT...|RED WOOLLY HOTTIE...|
  +--------------------+--------------------+
  ```

- translate 함수를 사용해 주어진 문자를 다른 문자로 치환할 수 있습니다. 이 연산은 문자 단위로 이루어집니다. 교체 문자열에서 색인된 문자에 해당하는 모든 문자를 치환합니다.

  ```scala
  import org.apache.spark.sql.functions.translate
  
  df.select(translate(col("Description"), "LEET", "1337").alias("translated"), col("Description")).show(2, false)
  
  +----------------------------------+----------------------------------+
  |translated                        |Description                       |
  +----------------------------------+----------------------------------+
  |WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|
  |WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |
  +----------------------------------+----------------------------------+
  ```

- 처음 나타난 색상 이름을 추출하는 작업을 수행할 때 regexp_extract를 사용할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.regexp_extract
  
  val regexString = simpleColors.map(_.toUpperCase).mkString("(", "|", ")")
  
  df.select(
    regexp_extract(col("Description"), regexString, 1).alias("color_clean"), col("Description")
  ).show(5, false)
  ```

- 단순히 값의 존재 여부를 확인하고 싶을 때는 contains 메소드를 사용합니다. 인수로 입력된 값이 컬럼의 문자열에 존재하는 지 불리언 타입으로 반환합니다.

  ```scala
  val containsBlack = col("Description").contains("BLACK")
  val containsWhite = col("Description").contains("WHITE")
  
  df.withColumn("hasSimpleColor", containsBlack.or(containsWhite))
    .where("hasSimpleColor")
    .select("Description").show(3, false)
  ```

- 파이썬, SQL에서는 instr함수를 사용해 값의 존재 여부를 확인합니다.

  ```python
  from pyspark.sql.functions import instr
  
  containsBlack = instr(col("Description"), "BLACK") >= 1
  containsWhite = instr(col("Description"), "WHITE") >= 1
  df.withColumn("hasSimpleColor", containsBlack | containsWhite)
    .where("hasSimpleColor").select("Description").show(3, False)
  ```

  ```sql
  select Description from dfTable
  where instr(Description, "BLACK") >= 1 or instr(Description, "WHITE") >= 1
  ```

- 동적으로 인수의 개수가 변하는 상황에서 스파큰는 값 목록을 인수로 변환해 함수에 전달할 때는 varargs라 불리는 스칼라 고유 기능을 활용합니다. 이 기능을 사용해서 임의 길이의 배열을 효율적으로 다룰 수 있습니다.

  ```scala
  val simpleColors = Seq("black", "white", "red", "green", "blue")
  
  val selectedColumns = simpleColors.map(color => {
    col("Description").contains(color.toUpperCase).alias(s"is_$color")
  }):+expr("*") // 이 값을 추가할 수도 있습니다.
  
  df.select(selectedColumns:_*).where(col("is_white").or(col("is_red"))).select("Description").show(3, false)
  ```

- 파이썬은 인수의 개수가 동적으로 변하는 상황을 쉽게 해결할 수 있습니다. 문자열의 위치를 정수형으로 반환하는 locate 함수를 사용합니다.

  ```python
  from pyspark.sql.functions import expr, locate
  
  simpleColors = ['black', 'white', 'red', 'green', 'blue']
  
  def color_locater(column, color_string):
      return locate(color_string.upper(), column)\
              .cast('boolean')\
              .alias("is_"+color_string)
  
  selectedColumns = [color_locater(df.Description, c) for c in simpleColors]
  selectedColumns.append(expr("*")) # Column 타입이어야 합니다.
  
  df.select(*selectedColumns).where(expr("is_white or is_red"))\
    .select("Description").show(3, False)
  ```


## 날짜와 타임스탬프 데이터 타입

- 스파크는 두 가지 종류의 시간 관련 정보만 집중적으로 관리합니다. 하나는 달력 형태의 날짜(date)이고, 다른 하나는 날짜와 시간 정보를 모두 가지는 타임스탬프입니다.

- 스파크는 inferSchema 옵션이 활성화되면 날짜와 타임스탬프를 포함해 컬럼의 데이터 타입을 최대한 정확하게 식별하려 합니다. 스파크는 특정 날짜 포맷을 명시하지 않아도 자체적으로 식별해 데이터를 읽을 수 있습니다.

- TimestampType 클래스는 초 단위까지만 지원합니다. 그러므로 밀리세컨드나 마이크로세컨드 단위를 다룬다면 long 데이터 타입으로 데이터를 변환해 처리하는 우회 정책을 사용해야 합니다.

- 스파크는 자바의 날짜와 타임스탬프를 사용해서 표준 체계를 따릅니다.

  ```scala
  import org.apache.spark.sql.functions.{current_date, current_timestamp}
  
  val dateDF = spark.range(10)
  				  .withColumn("today", current_date())
  				  .withColumn("now", current_timestamp())
  ```

- 오늘을 기준으로 5일 전후의 날짜를 구해 봅니다.

  ```scala
  import org.apache.spark.sql.functions.{date_add, date_sub, col}
  
  dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1, false)
  +------------------+------------------+
  |date_sub(today, 5)|date_add(today, 5)|
  +------------------+------------------+
  |2022-07-24        |2022-08-03        |
  +------------------+------------------+
  ```

- 두 날짜 사이의 일 수를 반환하는 datediff 함수

  ```scala
  import org.apache.spark.sql.functions.{datediff, months_between, to_date, lit}
  
  dateDF.withColumn("week_ago", date_sub(col("today"), 7))
        .select(datediff(col("week_ago"), col("today"))).show(1, false)
  
  +-------------------------+
  |datediff(week_ago, today)|
  +-------------------------+
  |-7                       |
  +-------------------------+
  ```

- to_date 함수는 문자열을 날짜로 변환할 수 있으며, 필요에 따라 날짜 포맷도 함께 지정할 수 있습니다. 함수의 날짜 포맷은 반드시 자바의 SimpleDateFormat 클래스가 지원하는 포맷을 사용해야 합니다.

  ```scala
  import org.apache.spark.sql.functions.{to_date, lit}
  
  spark.range(5)
       .withColumn("date", lit("2017-01-01"))
       .select(to_date(col("date"))).show(1, false)
  
  +-------------+
  |to_date(date)|
  +-------------+
  |2017-01-01   |
  +-------------+
  ```

- to_date함수는 필요에 따라 날짜 포맷을 지정할 수 있지만, to_timestamp 함수는 반드시 날짜 포맷을 지정해줘야 합니다.

  ```scala
  import org.apache.spark.sql.functions.to_timestamp
  
  cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()
  ```

## null 값 다루기

- 데이터프레임에서 빠져 있거나 비어 있는 데이터를 표현할 때는 항상 null 값을 사용하는 것이 좋습니다.

- coalesce

  - 스파크의 coalesce 함수는 인수로 지정한 여러 컬럼 중 null이 아닌 첫 번째 값을 반환합니다. 모든 컬럼이 null이 아닌 경우 첫 번째  컬럼의 값을 반환합니다.

    ```scala
    import org.apache.spark.sql.functions.coalesce
    
    df.select(coalesce(col("Description"), col("CustomerId"))).show(5, false)
    
    +-----------------------------------+
    |coalesce(Description, CustomerId)  |
    +-----------------------------------+
    |WHITE HANGING HEART T-LIGHT HOLDER |
    |WHITE METAL LANTERN                |
    |CREAM CUPID HEARTS COAT HANGER     |
    |KNITTED UNION FLAG HOT WATER BOTTLE|
    |RED WOOLLY HOTTIE WHITE HEART.     |
    +-----------------------------------+
    ```

- ifnull, nullif, nvl, nvl2

  - ifnull 함수는 첫 번째 값이 null이면 두 번째 값을 반환합니다. 첫 번째 값이 null이면 첫 번째 값을 반환합니다.

  - nullif 함수는 두 값이 같으면 null을 반환합니다. 두 값이 다르면 첫 번째 값을 반환합니다.

  - nvl 함수는 첫 번째 값이 null이면 두 번째 값을 반환합니다. 첫 번째 값이 null이 아니면 첫 번째 값을 반환합니다.

  - nvl2 함수는 첫 번째 값이 null이 아니면 두 번째 값을 반환합니다. 그리고 첫 번째 값이 null이면 세 번째 인수로 지정된 값을 반환합니다.

    ```sql
    select ifnull(null, 'return_value'), nullif('value', 'value'), nvl(null, 'return_value'), nvl2(null, 'return_value', 'else_value')
    from dfTable limit 1
    ```

- drop

  - null 값을 가진 로우를 제거하는 함수입니다. 기본적으로 null 값을 가진 모든 로우를 제거합니다.

  - drop 메소드의 인수로 any를 지정하면 로우의 컬럼값 중 하나라도 null이면 해당 로우를 제거합니다.

  - 인수를 all로 지정하면 로우의 컬럼 값이 모두 null이거나 NAN인 경우만 로우를 제거합니다.

  - drop 메소드에 배열 형태의 컬럼을 인수로 전달해 적용할 수도 있습니다.

    ```scala
    df.na.drop("all", Seq("StockCode", "InvoiceNo"))
    ```

- fill

  - fill 함수를 사용해 하나 이상의 컬럼을 특정 값으로 채울 수 있습니다. 채워 넣을 값과 컬럼 집합으로 구성된 맵을 인수로 사용합니다.

    ```scala
    df.na.fill(5, Seq("StockCode", "InvoiceNo"))
    ```

    ```python
    df.na.fill("all", subset=["StockCode", "InvoiceNo"])
    ```

  - 스칼라 Map 타입을 사용해 다수의 컬럼에 fill 메소드를 적용할 수도 있습니다. 이 때 key는 컬럼명이고, value는 null값을 채우는 데 사용할 값입니다.

    ```scala
    val fillColValues = Map("StockCode" -> 5, "Description" -> "No Value")
    df.na.fill(fillColValues)
    ```

- replace

  - replace 메소드를 사용하려면 변경하고자 하는 값과 원래 값의 데이터 타입이 같아야 합니다.

    ```scala
    df.na.replace("Description", Map("" -> "UNKNOWN"))
    ```

    ```python
    df.na.replace([""], ["UNKNOWN"], "Description")
    ```

## 정렬하기

- asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last 함수를 사용해 데이터프레임을 정렬 할 때 null 값이 표시되는 기준을 지정할 수 있습니다.

## 복합데이터 타입

- 구조체

  - 문법에 .을 사용하거나 getField 메소드를 사용할 수 있습니다.

    ```scala
    // complex에 있는 Description 필드 내용 출력
    complexDF.select("complex.Description")
    complexDF.select(col("complex").getField("Description"))
    ```

  - '*'문자를 사용해 모든 값을 조회할 수 있으며, 모든 컬럼을 데이터프레임의 최상위 수준으로 끌어올릴 수 있습니다.

    ```scala
    complexDF.select("complex.*")
    ```

- 배열

  - Description 컬럼을 복합 데이터 타입인 배열로 변환

    ```scala
    import org.apache.spark.sql.functions.split
    df.select(split(col("Description"), " ")).show(2, false)
    
    +----------------------------------------+
    |split(Description,  , -1)               |
    +----------------------------------------+
    |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|
    |[WHITE, METAL, LANTERN]                 |
    +----------------------------------------+
    ```

    ```sql
    select split(Description, ' ') from dfTable
    ```

  - split 함수는 스파크에서 복합 데이터 타입을 마치 또 다른 컬럼처럼 다룰 수 있는 강력한 기능입니다. 파이썬과 유사한 문법을 사용해 배열값을 조회할 수 있습니다.

    ```scala
    df.select(split(col("Description"), " ").alias("array_col"))
      .selelctExpr("array_col[0]").show(2, false)
    
    +------------+
    |array_col[0]|
    +------------+
    |WHITE       |
    |WHITE       |
    +------------+
    ```

  - 배열의 길이 (size)

    ```scala
    import org.apache.spark.sql.functions.size
    df.select(size(split(col("Description"), " "))).show(2,false)
    
    +-------------------------------+
    |size(split(Description,  , -1))|
    +-------------------------------+
    |5                              |
    |3                              |
    +-------------------------------+
    ```

  - array_contains

    - array_contains 함수를 사용해 배열에 특정 값이 존재하는 지 확인 할 수 있습니다.

      ```scala
      import org.apache.spark.sql.functions.array_contains
      
      df.select(array_contains(split(col("Description"), " "), "WHITE")).show(2, false)
      +------------------------------------------------+
      |array_contains(split(Description,  , -1), WHITE)|
      +------------------------------------------------+
      |true                                            |
      |true                                            |
      +------------------------------------------------+
      ```

      ```sql
      select array_contains(split(Description, ' '), 'WHITE') from dfTable
      ```

  - explode

    - explode 함수는 배열 타입의 컬럼을 입력 받습니다.  그리고 입력된 컬럼의 배열값에 포함된 모든 값을 로우로 변환합니다. 나머지 컬럼값은 중복되어 표시됩니다.

      ```scala
      import org.apache.spark.sql.functions.{split, explode}
      
      df.withColumn("splitted", split(col("Description"), " "))
        .withColumn("exploded", explode(col("splitted")))
        .select("Description", "InvoiceNo", "exploded").show(2, false)
      
      +----------------------------------+---------+--------+
      |Description                       |InvoiceNo|exploded|
      +----------------------------------+---------+--------+
      |WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |
      |WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |
      +----------------------------------+---------+--------+
      ```

- Map

  - map 함수와 컬럼의 키-값 쌍을 이용해 생성합니다. 그리고 배열과 동일한 방법으로 값을 선택할 수 있습니다.

    ```scala
    import org.apache.spark.sql.functions.map
    
    df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map")).show(2, false)
    
    +----------------------------------------------+
    |complex_map                                   |
    +----------------------------------------------+
    |{WHITE HANGING HEART T-LIGHT HOLDER -> 536365}|
    |{WHITE METAL LANTERN -> 536365}               |
    +----------------------------------------------+
    ```

  - 적합한 키를 사용해 데이터를 조회할 수 있으며, 해당 키가 없다면 null 값을 반환합니다.

    ```scala
    df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))
      .selectExpr("complex_map['WHITE METAL LANTERN']").show(2)
    
    +--------------------------------+
    |complex_map[WHITE METAL LANTERN]|
    +--------------------------------+
    |                            null|
    |                          536365|
    +--------------------------------+
    ```

  - map 타입은 분해하여 컬럼으로 변환할 수 있습니다.

    ```scala
    df.select(map(col("Description"), col("InvoiceNo")).alias("complex_map"))
      .selectExpr("explode(complex_map)").show(2, false)
    
    +----------------------------------+------+
    |key                               |value |
    +----------------------------------+------+
    |WHITE HANGING HEART T-LIGHT HOLDER|536365|
    |WHITE METAL LANTERN               |536365|
    +----------------------------------+------+
    ```

## JSON

- 스파크에서는 문자열 형태의 json을 직접 조작할 수 있으며, json을 파싱하거나 json 객체를 만들 수 있습니다.

  ```scala
  val jsonDF = spark.range(1).selectExpr("""'{"myJSONKey" : {"myJSONValue" : [1,2,3]}}' as jsonString""")
  ```

- get_json_object 함수로 json 객체를 인라인 쿼리로 조회 할 수 있습니다. 중첩이 없는 단일 수준의 json 객체라면 json_tuple을 사용할 수도 있습니다.

  ```scala
  import org.apache.spark.sql.functions.{get_json_object, json_tuple}
  
  jsonDF.select(
    get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]") as "Column",
    json_tuple(col("jsonString"), "myJSONKey")
  ).show(2, false)
  
  +------+-----------------------+
  |Column|c0                     |
  +------+-----------------------+
  |2     |{"myJSONValue":[1,2,3]}|
  +------+-----------------------+
  ```

  ```scala
  // spark SQL
  
  jsonDF.selectExpr(
    "get_json_object(jsonString, '$.myJSONKey.myJSONValue[1]') as column",
    "json_tuple(jsonString, 'myJSONKey')"
  ).show(2, false)
  ```

- to_json 함수를 사용해 StructType을 JSON 문자열로 변경할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.to_json
  
  df.selectExpr("(InvoiceNo, Description) as myStruct")
    .select(to_json(col("myStruct"))).show(2, false)
  
  +-------------------------------------------------------------------------+
  |to_json(myStruct)                                                        |
  +-------------------------------------------------------------------------+
  |{"InvoiceNo":"536365","Description":"WHITE HANGING HEART T-LIGHT HOLDER"}|
  |{"InvoiceNo":"536365","Description":"WHITE METAL LANTERN"}               |
  +-------------------------------------------------------------------------+
  ```

- from_json 함수를 사용해서 다시 객체로 변환할 수 있습니다. 이 때 from_json 함수는 파라미터로 반드시 스키마로 지정해야 합니다.

  ```scala
  import org.apache.spark.sql.functions.from_json
  import org.apache.spark.sql.types._
  
  val parseSchema = new StructType(Array(
    new StructField("InvoiceNo", StringType, true),
    new StructField("Description", StringType, true)
  ))
  
  df.selectExpr("(InvoiceNo, Description) as myStruct")
    .select(to_json(col("myStruct")).alias("newJSON"))
    .select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2,false)
  ```

## 사용자 정의 함수

- 스파크의 가장 강력한 기능 중 하나라고 할 수 있습니다.

- UDF는 파이썬이나 스칼라 그리고 외부 라이브러리를 사용해 사용자가 원하는 형태로 트랜스포메이션을 만들 수 있게 합니다.

- UDF는 레코드별로 데이터를 처리하는 함수이기 때문에 독특한 포맷이나 도메인에 특화된 언어를 사용하지 않습니다. 

- 이러한 UDF는 기본적으로 특정 SparkSession이나 Context에서 사용할 수 있도록 임시 함수 형태로 등록됩니다.

- 스파크는 드라이버에서 함수를 직렬화하고 네트워크를 통해 모든 익스큐터 프로세스로 전달합니다.

- 함수를 개발한 언어에 따라 근본적으로 동작 방식이 달라집니다. 스칼라나 자바로 함수를 작성했다면 JVM 환경에서만 사용할 수 있습니다. 따라서 약간의 성능 저하가 발생합니다.

- 파이썬으로 함수를 작성했다면 스파크는 워커노드에 파이썬 프로세스를 실행하고 파이썬이 이해할 수 있는 포맷으로 모든 데이터를 직렬화합니다. 그리고 파이썬 프로세스에 있는 데이터의 로우마다 함수를 실행하고 JVM과 스파크에 처리 결과를 반환합니다.

- 사용자 정의 함수를 스파크 SQL 함수로 등록하게 되면 모든 프로그래밍 언어와 sql에서 사용자 정의 함수를 사용할 수 있습니다.

  ```scala
  spark.udf.register("power3", power3(_:Double):Double)
  udfExampleDF.selectExpr("power3(num)").show(2, false)
  
  +-----------+
  |power3(num)|
  +-----------+
  |0.0        |
  |1.0        |
  +-----------+
  ```

  ```python
  from pyspark.sql.types import IntegerType, DoubleType
  
  spark.udf.register("power3py", power3, DoubleType())
  udfExampleDF.selectExpr("power3py(num)").show(2, false)
  
  +-------------+
  |power3py(num)|
  +-------------+
  |null         |
  |null         |
  +-------------+
  
  # null 값을 반환하는 이유는 range 메소드가 Integer 데이터 타입의 데이터를 만들었기 때문입니다.
  ```

  

- 스칼라로 개발된 사용자 정의 함수는 파이썬에서 우회적으로 사용할 수 있는 데, 그 이유는 데이터프레임에 스파크 SQL 함수나 SQL 표현식을 사용할 수 있기 때문입니다.

## Hive UDF

- 하이브 문법을 사용해서 만든 UDF/UDAF도 사용할 수 있습니다. 이렇게 하려면 SparkSession을 생성할 때 SparkSession.builder().enableHiveSupport()를 명시해 반드시 하이브 지원 기능을 활성화해야 합니다. 하이브 지원이 활성화되면 SQL로 UDF를 등록할 수 있습니다. 사전 컴파일된 스칼라와 자바 패키지에서만 지우되므로 라이브러리 의존성을 명시해야 합니다.

  ```sql
  create temporary function myFunc as 'com.organization.hive.udf.FunctionName'
  ```

  

