# 다양한 데이터 타입 다루기

## 스파크 데이터 타입으로 변환

- 데이터 타입 변환은 lit 함수를 사용합니다.

  ```scala
  import org.apache.spark.sql.functions.lit
  
  df.select(lit(5), lit("five"), lit(5.0))
  ```

## 불리언 데이터 타입 다루기

- 스칼라에서 ==, ===은 특별한 의미를 가집니다. 스파크에서 동등 여부를 판별해 필터링하면 일치를 나타내는 ===이나 불일치를 나타내는 =!=을 사용해야 합니다. not이나 equalTo를 사용할 수도 있습니다.

  ```scala
  import org.apache.spark.sql.functions.col
  
  // invoiceNo 값이 536365인 레코드 중 invoiceNo와 Description 컬럼 내용만 출력
  df.where(col("InvoiceNo") == 536365)
    .select("InvoiceNo", "Description")
    .show(5, false)
  ```

- 가장 명확한 방법은 문자열 표현식에 조건절을 명시하는 것입니다. 파이썬, 스칼라 모두에서 사용할 수 있습니다.

  ```scala
  df.where("InvoiceNo == 536365")
  df.where("InvoiceNo <> 536365")
  ```

- and, or 메소드를 사용해서 불리언 표현식을 여러 부분에 지정할 수 있습니다. 이 때 항상 모든 표현식을 and 메소드로 묶어서 차례대로 필터에 적용해야 합니다.

  ```scala
  val priceFilter = col("UnitPrice") > 600
  val descripFilter = col("Description").contains("POSTAGE")
  
  df.where(col("StockCode").isin("DOT")).where(priceFilter.or(descripFilter)).show()
  ```

  ```python
  from pyspark.sql.functions import instr
  # instr : 문자열에서 지정된 문자열을 검색해서 그 위치를 리턴하는 함수
  
  priceFilter = col("UnitPrice") > 600
  descripFilter = instr(df.Description, "POSTAGE") >= 1
  df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()
  ```

- 불리언 컬럼을 사용해서 데이터프레임을 필터링할 수 있습니다.

  ```scala
  val DOTCodeFilter = col("StockCode") === "DOT"
  val priceFilter = col("UnitPrice") > 600
  val descripFilter = col("Description").contains("POSTAGE")
  
  df.withColumn("isExpensive", DOTCodeFilter.and(priceFilter.or(descripFilter)))
    .where("isExpensive")
    .select("unitPrice", "isExpensive").show(5)
  ```

  ```python
  from pyspark.sql.functions import instr
  
  DOTCodeFilter = col("StockCode") == "DOT"
  priceFilter = col("UnitPrice") > 600
  descripFilter = instr(col("Description"), "POSTAGE") >= 1
  df.withColumn("isExpensive", DOTCodeFilter & (priceFilter | descripFilter)).where("isExpensive").select("unitPrice", "isExpensive").show(5)
  ```

- 필터를 표현할 때는 데이터프레임 인터페이스 방식보다 sql이 훨씬 쉽습니다.

  ```scala
  import org.apache.spark.sql.functions.{expr, not, col}
  
  // 두 코드는 같은 결과를 출력합니다.
  df.withColumn("isExpensive", not(col("UnitPrice").leq(250)))
    .filter("isExpensive")
    .select("Description", "UnitPrice").show(5)
  
  df.withColumn("isExpensive", expr("NOT UnitPrice <= 250"))
    .filter("isExpensive")
    .select("Description", "UnitPrice").show(5)
  ```

## 수치형 데이터 타입 다루기

- pow 함수

  ```scala
  import org.apache.spark.sql.functions.{expr, pow}
  
  val fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
  df.select(expr("CustomerID"), fabricatedQuantity.alias("realQuantity")).show(2)
  ```

  ```scala
  // spark sql code
  df.selectExpr(
    "CustomerId",
    "(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity"
  )show(2)
  ```

- 반올림 round 함수, 내림 bround 함수

  ```scala
  import org.apache.spark.sql.functions.lit
  
  df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
  ```

- StatFunctions 패키지는 다양한 통계함수를 제공합니다. stat 속성을 사용해서 접근할 수 있으며 다양한 통곗값을 계산할 때 사용하는 데이터프레임 메소드입니다.

  - approxQuantile

    ```scala
    val colName = "UnitPrice"
    val quantileProbs = Array(0.5)
    val relError = 0.05
    
    // 데이터의 백분위수 계산
    df.stat.approxQuantile("UnitPrice", quantileProbs, relError)
    ```

  - crosstab / freqItems

    ```scala
    df.stat.crosstab("StockCode", "Quantity").show()
    df.stat.freqItems(Seq("StockCode", "Quantity")).show()
    ```

  - monotonically_increasing_id : 모든 로우에 고유 ID 값을 추가

    ```scala
    import org.apache.spark.sql.functions.monotonically_increasing_id
    df.select(monotonically_increasing_id()).show(2)
    ```

## 문자열 데이터 타입 다루기

- 문자열을 다루는 작업은 거의 모든 데이터 처리 과정에서 발생합니다. 로그 파일에 정규 표현식을 사용해 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 처리 등의 작업을 할 수 있습니다.

- 대/소문자 변환 작업

  - initcap 함수는 주어진 문자열에서 공백으로 나뉘는 모든 단어의 첫 글자를 대문자로 변경합니다.

    ```scala
    import org.apache.spark.sql.functions.{initcap}
    
    df.select(initcap(col("Description"))).show(2, false)
    ```

  - lower 함수는 문자열 전체를 소문자로 변경하거나, upper 함수를 사용해 문자열 전체를 대문자로 변경할 수 있습니다.

    ```scala
    import org.apache.spark.sql.functions.{lower,upper}
    
    var col_obj = col("Description")
    df.select(col_obj, lower(col_obj), upper(col_obj)).show(2)
    ```

  - 문자열 주변의 공백을 제거하거나 추가하는 작업 : lpad, ltrim, rpad, rtrim, trim 함수를 사용합니다.

    ```scala
    import org.apache.spark.sql.functions.{lit, ltrim, rtrim, rpad, lpad, trim}
    
    df.select(
      ltrim(lit("    hello    ")).as("ltrim"),
      rtrim(lit("    hello    ")).as("rtrim"),
      trim(lit("    hello    ")).as("trim"),
      lpad(lit("hello"), 3, " ").as("lpad"),
      rpad(lit("hello"), 10, " ").as("rpad"),
    ).show(2)
    
    /*
    +---------+---------+-----+----+----------+
    |    ltrim|    rtrim| trim|lpad|      rpad|
    +---------+---------+-----+----+----------+
    |hello    |    hello|hello| hel|hello     |
    |hello    |    hello|hello| hel|hello     |
    +---------+---------+-----+----+----------+
    */
    ```

## 정규표현식

- 문자열의 존재 여부를 확인하거나 일치하는 모든 문자열을 치환할 때 보통 정규 표현식을 사용합니다. 정규 표현식을 사용해 문자열에서 값을 추출하거나 다른 값으로 치환하는 데 필요한 규칙 모음을 정의할 수 있습니다.

- 스파크는 정규 표현식을 위해 regexp_extract 함수와 regexp_replace 함수를 제공합니다.

  ```scala
  // regexp_replace 함수를 사용해 'description' 컬럼의 값을 'COLOR'로 치환해본다.
  
  import org.apache.spark.sql.functions.regexp_replace
  
  val simpleColors = Seq("black", "white", "red", "green", "blue")
  val regexString = simpleColors.map(_.toUpperCase).mkString("|")
  
  df.select(
      regexp_replace(col("Description"), regexString, "COLOR").alias("color_clean"),
      col("Description")
  ).show(5)
  
  +--------------------+--------------------+
  |         color_clean|         Description|
  +--------------------+--------------------+
  |COLOR HANGING HEA...|WHITE HANGING HEA...|
  | COLOR METAL LANTERN| WHITE METAL LANTERN|
  |CREAM CUPID HEART...|CREAM CUPID HEART...|
  |KNITTED UNION FLA...|KNITTED UNION FLA...|
  |COLOR WOOLLY HOTT...|RED WOOLLY HOTTIE...|
  +--------------------+--------------------+
  ```

- translate 함수를 사용해 주어진 문자를 다른 문자로 치환할 수 있습니다. 이 연산은 문자 단위로 이루어집니다. 교체 문자열에서 색인된 문자에 해당하는 모든 문자를 치환합니다.

  ```scala
  import org.apache.spark.sql.functions.translate
  
  df.select(translate(col("Description"), "LEET", "1337").alias("translated"), col("Description")).show(2, false)
  
  +----------------------------------+----------------------------------+
  |translated                        |Description                       |
  +----------------------------------+----------------------------------+
  |WHI73 HANGING H3AR7 7-1IGH7 HO1D3R|WHITE HANGING HEART T-LIGHT HOLDER|
  |WHI73 M37A1 1AN73RN               |WHITE METAL LANTERN               |
  +----------------------------------+----------------------------------+
  ```

- 처음 나타난 색상 이름을 추출하는 작업을 수행할 때 regexp_extract를 사용할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.regexp_extract
  
  val regexString = simpleColors.map(_.toUpperCase).mkString("(", "|", ")")
  
  df.select(
    regexp_extract(col("Description"), regexString, 1).alias("color_clean"), col("Description")
  ).show(5, false)
  ```

- 단순히 값의 존재 여부를 확인하고 싶을 때는 contains 메소드를 사용합니다. 인수로 입력된 값이 컬럼의 문자열에 존재하는 지 불리언 타입으로 반환합니다.

  ```scala
  val containsBlack = col("Description").contains("BLACK")
  val containsWhite = col("Description").contains("WHITE")
  
  df.withColumn("hasSimpleColor", containsBlack.or(containsWhite))
    .where("hasSimpleColor")
    .select("Description").show(3, false)
  ```

- 파이썬, SQL에서는 instr함수를 사용해 값의 존재 여부를 확인합니다.

  ```python
  from pyspark.sql.functions import instr
  
  containsBlack = instr(col("Description"), "BLACK") >= 1
  containsWhite = instr(col("Description"), "WHITE") >= 1
  df.withColumn("hasSimpleColor", containsBlack | containsWhite)
    .where("hasSimpleColor").select("Description").show(3, False)
  ```

  ```sql
  select Description from dfTable
  where instr(Description, "BLACK") >= 1 or instr(Description, "WHITE") >= 1
  ```

- 동적으로 인수의 개수가 변하는 상황에서 스파큰는 값 목록을 인수로 변환해 함수에 전달할 때는 varargs라 불리는 스칼라 고유 기능을 활용합니다. 이 기능을 사용해서 임의 길이의 배열을 효율적으로 다룰 수 있습니다.

  ```scala
  val simpleColors = Seq("black", "white", "red", "green", "blue")
  
  val selectedColumns = simpleColors.map(color => {
    col("Description").contains(color.toUpperCase).alias(s"is_$color")
  }):+expr("*") // 이 값을 추가할 수도 있습니다.
  
  df.select(selectedColumns:_*).where(col("is_white").or(col("is_red"))).select("Description").show(3, false)
  ```

- 파이썬은 인수의 개수가 동적으로 변하는 상황을 쉽게 해결할 수 있습니다. 문자열의 위치를 정수형으로 반환하는 locate 함수를 사용합니다.

  ```python
  from pyspark.sql.functions import expr, locate
  
  simpleColors = ['black', 'white', 'red', 'green', 'blue']
  
  def color_locater(column, color_string):
      return locate(color_string.upper(), column)\
              .cast('boolean')\
              .alias("is_"+color_string)
  
  selectedColumns = [color_locater(df.Description, c) for c in simpleColors]
  selectedColumns.append(expr("*")) # Column 타입이어야 합니다.
  
  df.select(*selectedColumns).where(expr("is_white or is_red"))\
    .select("Description").show(3, False)
  ```

  

