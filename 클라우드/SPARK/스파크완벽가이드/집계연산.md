# 집계 연산

- group by는 하나 이상의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있습니다.
- 윈도우는 하나 이상의 키를 지정할 수 있으며, 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있습니다. 하지만 함수의 입력으로 사용할 로우는 현재 로우와 어느 정도 연관이 있어야 합니다.
- 그룹화 셋(grouping set)은 서로 다른 레벨의 값을 집계할 때 사용합니다. SQL, 데이터프레임의 롤업 그리고 큐브를 사용할 수 있습니다.
- 롤업은 하나 이상의 킬르 지정할 수 있습니다. 그리고 컬럼을 변환하는 데 다른 집계 함수를 사용해 계층적으로 요약된 값을 구할 수 있습니다.
- 큐브는 하나 이상의 키를 지정할 수 있으며 값을 가진 컬럼을 변환하기 위해 다른 집계 함수를 사용할 수 있습니다. 큐브는 모든 컬럼 조합에 대한 요약 값을 계산합니다.



## count

- count 함수에 특정 컬럼을 지정하는 방식

- count(*)나 count(1)을 사용하는 방식

  ```scala
  import org.apache.spark.sql.functions.count
  
  df.select(count("StockCode")).show() // 541909
  ```

- count(*) 구문을 사용하면 null 값을 가진 로우를 포함해 카운트합니다. 하지만 count 함수에 특정 컬럼을 지정하면 null 값을 카운트하지 않습니다.

## countDistinct

- 전체 레코드 수가 아닌 고유 레코드 수를 구해야 할 때 사용합니다.

  ```scala
  import org.apache.spark.sql.functions.countDistinct
  
  df.select(countDistinct("StockCode")).show() //4070
  ```

## approx_count_distinct

- 정확한 고유 개수가 아니고 근사치만으로도 유의미할 때 사용할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.approx_count_distinct
  df.select(approx_count_distinct("StockCode", 0.1)).show() //3364
  ```

- 해당 함수는 최대 추정 오류율이라는 한가지 파라미터를 더 사용합니다.

- 이 함수의 성능은 대규모 데이터셋을 사용할 때 훨씬 더 좋아집니다.

## first, last

- 데이터 프레임의 첫 번째 값이나 마지막 값을 얻을 때 사용합니다.

- 데이터 프레임의 값이 아닌 로우를 기반으로 동작합니다.

  ```scala
  import org.apache.spark.sql.functions.{first, last}
  
  df.select(first("StockCode"), last("StockCode")).show()
  
  +----------------+---------------+
  |first(StockCode)|last(StockCode)|
  +----------------+---------------+
  |          85123A|          22138|
  +----------------+---------------+
  ```

## min, max

- 데이터프레임에서 최솟값과 최댓값을 추출할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.{min, max}
  
  df.select(min("Quantity"), max("Quantity")).show()
  
  +-------------+-------------+
  |min(Quantity)|max(Quantity)|
  +-------------+-------------+
  |       -80995|        80995|
  +-------------+-------------+
  ```

## sum

- 데이터프레임에서 특정 컬럼의 모든 값을 합산할 때 사용합니다

  ```scala
  import org.apache.spark.sql.functions.sum
  
  df.select(sum("Quantity")).show() //5176450
  ```

## sumDistinct

- 특정 컬럼의 고윳값을 합산 할 때 사용합니다.

  ```scala
  import org.apache.spark.sql.functions.sumDistinct
  
  df.select(sumDistinct("Quantity")).show() //29310
  ```

## 분산과 표준편차

- 스파크는 함수를 이용해 분산과 표준편차를 계산할 수 있습니다.

- variance나 stddev 함수를 사용한다면 기본적으로 표본 표준분산과 표본 표준편차 공식을 이용합니다.

- 모표준분산이나 모표준편차 방식을 사용하려면 var_pop, stddev_pop 함수를 사용합니다.

  ```scala
  import org.apache.spark.sql.functions.{var_pop, stddev_pop}
  import org.apache.spark.sql.functions.{var_samp, stddev_samp}
  
  df.select(var_pop("Quantity"), var_samp("Quantity"),
           stddev_pop("Quantity"), stddev_pop("Quantity")).show()
  +-----------------+------------------+--------------------+--------------------+
  |var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_pop(Quantity)|
  +-----------------+------------------+--------------------+--------------------+
  |47559.30364660917| 47559.39140929886|   218.0809566344782|   218.0809566344782|
  +-----------------+------------------+--------------------+--------------------+
  
  ```

## 비대칭도와 첨도

- 데이터의 변곡점을 측정하는 방법입니다.

- 비대칭도는 데이터 평균의 비대칭 정도를 측정하고, 첨도는 데이터 끝 부분을 측정합니다.

- 확률변수와 확률분포로 데이터를 모델링할 때 중요합니다.

  ```scala
  import org.apache.spark.sql.functions.{skewness, kurtosis}
  df.select(skewness("Quantity"), kurtosis("Quantity")).show()
  
  +--------------------+------------------+
  |  skewness(Quantity)|kurtosis(Quantity)|
  +--------------------+------------------+
  |-0.26407557610528065|119768.05495536947|
  +--------------------+------------------+
  ```

## 공분산과 상관관계

- cov, corr 함수를 사용해 공분산과 상관관계를 계산할 수 있습니다.

- 공분산은 데이터 입력값에 따라 다른 범위를 가집니다. 상관관계는 피어슨 상관계수를 측정하며 -1과 1사이의 값을 가집니다.

  ```scala
  import org.apache.spark.sql.functions.{corr, covar_pop, covar_samp}
  
  df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"), covar_pop("InvoiceNo", "Quantity")).show()
  
  +-------------------------+-------------------------------+------------------------------+
  |corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|
  +-------------------------+-------------------------------+------------------------------+
  |     4.912186085637207E-4|              1052.728054390769|            1052.7260778746647|
  +-------------------------+-------------------------------+------------------------------+
  ```



## 복합 데이터 타입 집계

- 특정 컬럼의 값을 리스트로 수집하거나 셋 데이터 타입으로 고윳값만 수집할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.{collect_set, collect_list}
  
  df.agg(collect_set("Country"), collect_list("Country")).show()
  ```



## 그룹화

- 데이터 그룹 기반의 집계는 단일 컬럼의 데이터를 그룹화하고 해당 그룹의 다른 여러 컬럼을 사용해서 계산하기 위해 카테고리형 데이터를 사용합니다.

  ```scala
  df.groupBy("InvoiceNo", "CustomerId").count().show(2)
  
  +---------+----------+-----+
  |InvoiceNo|CustomerId|count|
  +---------+----------+-----+
  |   536846|     14573|   76|
  |   537026|     12395|   12|
  +---------+----------+-----+
  ```

## 표현식을 이용한 그룹화

- 카운팅을 메소드 대신 count 함수를 사용하는 것이 더 좋습니다.

- count 함수를 select 구문에 표현식으로 쓰는 것보다 agg 메소드를 사용하는 것이 좋습니다.

- agg 메소드는 여러 집계 처리를 한번에 지정할 수 있고, 집계에 표현식을 사용할 수 있습니다.

- 트랜스포메이션이 완료된 컬럼에 alias 메소드를 적용할 수 있습니다.

  ```scala
  import org.apache.spark.sql.functions.count
  
  df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)")
  ).show(5)
  
  +---------+----+---------------+
  |InvoiceNo|quan|count(Quantity)|
  +---------+----+---------------+
  |   536596|   6|              6|
  |   536938|  14|             14|
  |   537252|   1|              1|
  |   537691|  20|             20|
  |   538041|   1|              1|
  +---------+----+---------------+
  ```

## 맵을 이용한 그룹화

- 컬럼을 키로, 수행할 집계 함수의 문자열을 값으로 하는 map 타입을 사용해 트랜스포메이션을 정의할 수 있습니다.

- 수행할 집계 함수를 한 줄로 작성하면 여러 컬럼명을 재사용할 수 있습니다.

  ```scala
  df.groupBy("InvoiceNo").agg("Quantity"->"avg", "Quantity"->"stddev_pop").show(5)
  
  +---------+------------------+--------------------+
  |InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|
  +---------+------------------+--------------------+
  |   536596|               1.5|  1.1180339887498947|
  |   536938|33.142857142857146|  20.698023172885524|
  |   537252|              31.0|                 0.0|
  |   537691|              8.15|   5.597097462078001|
  |   538041|              30.0|                 0.0|
  +---------+------------------+--------------------+
  ```

## 윈도우 함수

- 데이터의 특정 윈도우를 대상으로 고유의 집계 연산을 수행합니다

- 데이터의 윈도우는 현재 데이터에 대한 참조를 사용해 정의합니다.

- group-by 함수와 차이점

  - group-by 함수를 사용하면 모든 로우 레코드가 단일 그룹으로 이동합니다
  - 윈도우 함수는 프레임에 입력되는 모든 로우에 대해 결괏값을 계산합니다.

- 윈도우 함수를 정의하기 위해서는 첫 번째로 윈도우 명세를 만들어야 합니다.

- partitionBy, orderBy, rowsBetween

  ```scala
  import org.apache.spark.sql.expressions.Window
  import org.apache.spark.sql.functions.col
  
  val windowSpec = Window
          .partitionBy("CustomerId", "date") // 그룹을 어떻게 나눌지 결정
          .orderBy(col("Quantity").desc) // 파티션의 정렬 방식 정의
          .rowsBetween(Window.unboundedPreceding, Window.currentRow) //로우가 프레임에 포함될 수 있는 지
  ```

- 예제) 시간대별 최대 구매 개수 구하기

  ```scala
  import org.apache.spark.sql.functions.max
  
  val maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)
  ```

- 모든 고객에 대해 최대 구매 수량을 가진 날짜가 언제인 지

  ```scala
  import org.apache.spark.sql.functions.col
  spark.sql("set spark.sql.legacy.timeParserPolicy=LEGACY")
  
  dfWithDate.where("CustomerId is not null").orderBy("CustomerId")
            .select(
              col("CustomerId"),
              col("date"),
              col("Quantity"),
              purchaseRank.alias("quantityRank"),
              purchaseDenseRank.alias("quantityDenseRank"),
              maxPurchaseQuantity.alias("maxPurchaseQuantity")
            ).show()
  ```



## 그룹화 셋

- 여러 그룹에 걸쳐 집계할 때 사용할 수 있습니다.

- 그룹화 셋은 여러 집계를 결합하는 저수준 기능입니다.

- 예제) stock code와 cutomerid 별 총 수량을 구하는 코드

  ```sql
  select CustomerId, stockCode, sum(Quantity) from dfNoNull
  group by customerId, stockCode
  order by customerid desc, stockcode desc
  ```

  ```sql
  -- 그룹화 셋을 사용해 동일한 작업 수행
  select customerId, stockCode, sum(Quantity) from dfNoNull
  group by customerId, stockCode, grouping sets((customerId, stockCode))
  order by customerId desc, stockCode desc
  ```

- 고객이나 재고 코드에 상관없이 총 수량의 합산을 추가하는 건 group-by 구문으로 처리하는 것이 불가능하기 때문에 그룹화 셋을 사용해야 합니다.

  ```sql
  select customerid, stockcode, sum(quantity) from dfNoNull
  group by customerid, stockcode grouping sets((customerid, stockcode), ())
  order by sum(quantity) desc, customerid desc, stockcode desc
  ```

- grouping sets 구문은 sql에서만 사용할 수 있습니다. 데이터프레임에서 동일한 연산을 수행하려면 rollup 메소드와 cube 메소드를 사용합니다.



## roll up

- 롤업은 group-by 스타일의 다양한 연산을 수행할 수 있는 다차원 집계 기능입니다.

- 예제) 시간(Date)과 공간(Country)을 축으로 하는 롤업을 생성

  ```scala
  val rolledUpDF = dfNoNull.rollup("Date", "Country").agg(sum("Quantity"))
                           .selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity")
                           .orderBy("Date")
  
  ```

## Cube

- 큐브는 롤업을 고차원적으로 사용할 수 있게 해줍니다. 큐브는 요소들을 계층적으로 다루는 대신 모든 차원에 동일한 작업을 수행합니다. 즉 전체 기간에 대해 날짜와 국가별 결과를 얻을 수 있습니다.

  ```scala
  dfNoNull.cube("Date", "Country").agg(sum(col("Quantity"))).select("Date", "Country", "sum(Quantity)")
          .orderBy("Date").show(5)
  
  +----+---------+-------------+
  |Date|  Country|sum(Quantity)|
  +----+---------+-------------+
  |null|  Iceland|         2458|
  |null|  Germany|       117448|
  |null|Lithuania|          652|
  |null|Singapore|         5234|
  |null|  Finland|        10666|
  +----+---------+-------------+
  // 전체 기간이기 때문에 date 값은 모두 null
  ```

## 그룹화 메타 데이터

- 큐브와 롤업을 사용하면 집계 수준에 따라 쉽게 필터링하기 위해 집계 수준을 조회하는 경우가 발생합니다. 이 때 grouping_id를 사용합니다. grouping_id는 결과 데이터셋의 집계수준을 명시하는 컬럼을 제공합니다.

  | 그룹화 ID | 설명                                                         |
  | --------- | ------------------------------------------------------------ |
  | 3         | 가장 높은 계층의 집계 결과에서 나타납니다. customerId나 StockCode에 관계없이 총 수량을 제공합니다. |
  | 2         | 개별 Stock Code의 모든 집계 결과에서 나타납니다. customerId에 관계없이 StockCode 별 총 수량을 제공합니다. |
  | 1         | 구매한 물품에 관계 없이 customerId를 기반으로 총 수량을 제공합니다 |
  | 0         | customerId와 stockCode별 조합에 따라 총 수량을 제공합니다.   |

  ```scala
  import org.apache.spark.sql.functions.{grouping_id, sum, expr}
  
  dfNoNull.cube("customerId", "stockCode").agg(grouping_id(), sum("Quantity"))
          .orderBy(col("grouping_id()").desc)
          .show(5)
  
  +----------+---------+-------------+-------------+
  |customerId|stockCode|grouping_id()|sum(Quantity)|
  +----------+---------+-------------+-------------+
  |      null|     null|            3|      4906888|
  |      null|    22065|            2|         6430|
  |      null|   90214M|            2|           27|
  |      null|    20713|            2|        10223|
  |      null|    22642|            2|          253|
  +----------+---------+-------------+-------------+
  ```

## 피벗

- 피벗을 사용해 로우를 컬럼으로 변환할 수 있습니다.

- 예제) 피벗을 사용해 국가별로 집계 함수를 적용

  ```scala
  // 피벗 함수를 이용해 날짜 별로 나라 별 합계를 구할 수 있다.
  
  val pivoted = dfWithDate.groupBy("date").pivot("Country").sum()
  ```

  ```scala
  pivoted.where("date > '2011-12-05'").select("date", "`USA_sum(Quantity)`").show()
  
  +----------+-----------------+
  |      date|USA_sum(Quantity)|
  +----------+-----------------+
  |2011-12-06|             null|
  |2011-12-09|             null|
  |2011-12-08|             -196|
  |2011-12-07|             null|
  +----------+-----------------+
  ```

- 특정 컬럼의 카디널리티가 낮다면 스키마와 쿼리 대상을 확인할 수 있도록 피벗을 사용해 다수의 컬럼으로 변환하는 것이 좋습니다.



## 사용자 정의 집계 함수

- UDAF는 직접 제작한 함수나 비즈니스 규칙에 기반을 둔 자체 집계 함수를 정의하는 방법입니다.

- 스파크는 입력 데이터의 모든 그룹의 중간 결과를 단일 AggregationBuffer에 저장해 관리합니다.

- UDAF를 생성하려면 기본 클래스인 UserDefinedAggregateFunction을 상속받습니다. 그리고 다음 메소드를 정의해야 합니다.

  - inputSchema
  - bufferSchema
  - dataType
  - deterministic
  - initialize
  - update
  - merge
  - evaluate

- 예제) 입력된 모든 로우의 컬럼이 true인지 아닌지 판단하는 BoolAnd 클래스를 구현

  ```scala
  
  ```

  



